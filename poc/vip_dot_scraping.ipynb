{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf12c8a",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import pdfplumber\n",
    "import tabula\n",
    "from pdfminer.high_level import extract_text\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from selenium.common.exceptions import InvalidArgumentException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pyautogui\n",
    "from email.mime.text import MIMEText\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pytz\n",
    "import logging\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9cb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181b740",
   "metadata": {},
   "source": [
    "Miscellanious Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information from a duckdb file\n",
    "def get_data(folder,duckdb_file,table_name):\n",
    "    folder = folder\n",
    "    duckdb_file = duckdb_file\n",
    "    db_file = rf\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\{folder}\\{duckdb_file}.duckdb\"\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(db_file)\n",
    "    # Current scraped data\n",
    "    table_name = table_name\n",
    "    data = con.execute(f\"SELECT * FROM {table_name}\").fetch_df()\n",
    "    con.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1e7f0",
   "metadata": {},
   "source": [
    "California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4594f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # URL of the site\n",
    "url = 'https://dot.ca.gov/programs/construction/statement-of-ongoing-contracts'\n",
    "\n",
    "# Start a new browser session\n",
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "year = now.year\n",
    "month = now.strftime('%B') \n",
    "prev_month_date = now - relativedelta(months=1)\n",
    "prev_month = prev_month_date.strftime('%B')\n",
    "prev_month_year = prev_month_date.year \n",
    "\n",
    "try:\n",
    "    try:\n",
    "        current_month_report = WebDriverWait(driver, 15).until(\n",
    "                EC.visibility_of_element_located((By.LINK_TEXT, f'Ongoing Contracts as of {month} {year} Alt Format (XLSX)'))\n",
    "        )\n",
    "        current_month_report.click()\n",
    "        time.sleep(5)\n",
    "    except TimeoutException:\n",
    "        print(f\"Monthly report for {month} {year} not yet available. Check to confirm.\")\n",
    "        \n",
    "    previous_month_report = WebDriverWait(driver, 15).until(\n",
    "                EC.visibility_of_element_located((By.LINK_TEXT, f'Ongoing Contracts as of {prev_month} {prev_month_year} Alt Format (XLSX)'))\n",
    "    )\n",
    "    previous_month_report.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "except TimeoutException:\n",
    "    print(f\"Monthly report for {prev_month} {prev_month_year} not yet available. Check manually to confirm.\")\n",
    "    \n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f15a65",
   "metadata": {},
   "source": [
    "Colorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf31444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from all pages except for page 1\n",
    "\n",
    "# Regex to match quantities with exactly three decimal places (e.g., \"244.000\")\n",
    "qty_re = re.compile(r'^[\\d,]+\\.\\d{3}$')\n",
    "# General numeric regex (allows any decimal places)\n",
    "numeric_re = re.compile(r'^-?[\\d,]*\\.\\d+$')\n",
    "\n",
    "def extract_pdf_to_dataframe(pdf_path):\n",
    "    records = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        # Skip the very first page\n",
    "        for page in pdf.pages[1:]:\n",
    "            text = page.extract_text() or \"\"\n",
    "            lines = [ln.strip() for ln in text.split('\\n') if ln.strip()]\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                tokens1 = lines[i].split()\n",
    "                # Identify a line beginning with a 4-digit REF NO\n",
    "                if len(tokens1) >= 4 and re.match(r'^\\d{4}$', tokens1[0]):\n",
    "                    # Find tokens on line1 that match the 3-decimal pattern\n",
    "                    qty_idxs = [idx for idx,t in enumerate(tokens1) if qty_re.match(t)]\n",
    "                    if len(qty_idxs) < 2:\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    # Extract REF, ITEM, CURRENT and PREV quantities\n",
    "                    ref_no      = tokens1[0]\n",
    "                    item_no     = tokens1[1]\n",
    "                    current_qty = tokens1[qty_idxs[-2]]\n",
    "                    prev_qty    = tokens1[qty_idxs[-1]]\n",
    "\n",
    "                    # Everything between token 2 and the first qty token is the description\n",
    "                    desc_parts = tokens1[2: qty_idxs[-2]]\n",
    "                    desc_lines = [\" \".join(desc_parts)]\n",
    "\n",
    "                    # --- Locate the line with unit_price (Line-3) ---\n",
    "                    idx_line3 = None\n",
    "                    for k in range(i+1, len(lines)):\n",
    "                        first_tok = lines[k].split()[0]\n",
    "                        if numeric_re.match(first_tok):\n",
    "                            idx_line3 = k\n",
    "                            break\n",
    "                    if idx_line3 is None or idx_line3 - i < 2:\n",
    "                        i += 1\n",
    "                        continue\n",
    "                    idx_line2 = idx_line3 - 1\n",
    "\n",
    "                    # Merge any wrapped description lines between 1 & 2\n",
    "                    for k in range(i+1, idx_line2):\n",
    "                        first_tok = lines[k].split()[0]\n",
    "                        if not numeric_re.match(first_tok):\n",
    "                            desc_lines.append(lines[k])\n",
    "                    full_description = \" \".join(desc_lines)\n",
    "\n",
    "                    # --- Parse Line-2: units, qty this period, amount this period ---\n",
    "                    t2 = lines[idx_line2].split()\n",
    "                    num2_idxs = [j for j,t in enumerate(t2) if numeric_re.match(t)]\n",
    "                    if len(num2_idxs) >= 2:\n",
    "                        first2 = num2_idxs[0]\n",
    "                        units       = \" \".join(t2[:first2])\n",
    "                        qty_this    = t2[num2_idxs[0]]\n",
    "                        amt_this    = t2[num2_idxs[1]]\n",
    "                    else:\n",
    "                        units = qty_this = amt_this = \"\"\n",
    "\n",
    "                    # --- Parse Line-3: unit_price, qty_to_date, cumulative_amount ---\n",
    "                    t3 = lines[idx_line3].split()\n",
    "                    num3 = [tok for tok in t3 if numeric_re.match(tok) or tok.startswith(\"-\")]\n",
    "                    unit_price        = num3[0] if len(num3)>0 else \"\"\n",
    "                    qty_to_date       = num3[1] if len(num3)>1 else \"\"\n",
    "                    cumulative_amount = num3[2] if len(num3)>2 else \"\"\n",
    "\n",
    "                    # Combine fields\n",
    "                    current_combined = f\"{current_qty} {units} {unit_price}\".strip()\n",
    "                    prev_combined    = f\"{prev_qty} {qty_this} {qty_to_date}\".strip()\n",
    "\n",
    "                    records.append({\n",
    "                        \"REF NO.\": ref_no,\n",
    "                        \"ITEM NO.\": item_no,\n",
    "                        \"ITEM DESCRIPTION\": full_description,\n",
    "                        \"CURRENT QUANTITY/UNITS UNIT PRICE\": current_combined,\n",
    "                        \"PREV QTY/ QTY THIS PERIOD QTY TO DATE\": prev_combined,\n",
    "                        \"AMOUNT THIS PERIOD\": amt_this,\n",
    "                        \"CUMULATIVE AMOUNT\": cumulative_amount\n",
    "                    })\n",
    "\n",
    "                    # Advance past this block\n",
    "                    i = idx_line3 + 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Colorado\\Example_11582_27.pdf\"\n",
    "    df = extract_pdf_to_dataframe(file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c90785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from page 1 only\n",
    "# --- CONFIG ---\n",
    "single_fields = [\n",
    "    \"CONTID\", \"PCN\", \"ESTIMATE NO\", \"SPEC YR\", \"COFRS REPORTING CATEGORY\",\n",
    "    \"FACS REF NO\", \"CONTRACT DESCRIPTION\", \"TIME CHARGED\", \"TIME ALLOW\",\n",
    "    \"PERCENT TIME\", \"ORIG TIME ALLOW\", \"PROJECT NO\", \"NAME OF ROAD\",\n",
    "    \"PROJECT COUNTIES\", \"PAY PERIOD ENDING\", \"DATE TIME STARTED\",\n",
    "    \"DATE LET\", \"DATE WORK BEGAN\", \"DATE AWARDED\", \"DATE TIME STOPPED\",\n",
    "    \"DATE CONTRACT EXECUTED\", \"DATE ACCEPTED\", \"DATE NOTICE TO PROCEED\",\n",
    "    \"CURRENT PROJECT AMT\", \"AWARD PROJECT AMT\", \"PERCENT COMPLETE\",\n",
    "    \"FUNDS AVAILABLE\", \"TOTAL CLAIMS\", \"APPROVED FOR PAYMENT BY\",\n",
    "    \"PAYMENT DUE\", \"PROJECT COMMENT\", \"ESTIMATE COMMENT\"\n",
    "]\n",
    "two_col_fields = [\n",
    "    \"PARTICIPATING\", \"NON-PARTICIPATING\", \"TOTAL EARNINGS\",\n",
    "    \"STOCKPILED MATERIALS\", \"GROSS EARNINGS\", \"RETAINAGE\",\n",
    "    \"SECURITIES ENCUMBERED\", \"NET EARNINGS\", \"LIQUIDATED DAMAGES\",\n",
    "    \"AUTOPAY ADJUSTMENT\", \"AMOUNT DUE\", \"OTHER ADJUSTMENTS\"\n",
    "]\n",
    "expanded_two_col = [f\"{f}_{suf}\" for f in two_col_fields for suf in (\"CURRENT_TOTAL\",\"THIS_ESTIMATE\")]\n",
    "ALL_COLUMNS = single_fields + expanded_two_col\n",
    "\n",
    "def extract_page1(pdf_path: str) -> pd.DataFrame:\n",
    "    # helper to avoid None.group errors\n",
    "    def safe_search(pattern, txt):\n",
    "        m = re.search(pattern, txt)\n",
    "        return m.group(1) if m else \"\"\n",
    "    \n",
    "    # read page1\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = pdf.pages[0].extract_text()\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # prepare empty row\n",
    "    row = {col: \"\" for col in ALL_COLUMNS}\n",
    "    \n",
    "    # --- SINGLE FIELDS ---\n",
    "    l3 = lines[3] if len(lines)>3 else \"\"\n",
    "    row[\"CONTID\"]  = safe_search(r\"CONTID:(\\S+)\", l3)\n",
    "    row[\"ESTIMATE NO\"] = safe_search(r\"ESTIMATE NO:(\\S+)\", l3)\n",
    "    row[\"SPEC YR\"] = safe_search(r\"FINL SPEC YR:(\\d+)\", l3)\n",
    "    \n",
    "    l4 = lines[4] if len(lines)>4 else \"\"\n",
    "    row[\"PCN\"]      = safe_search(r\"PCN:(\\S+)\", l4)\n",
    "    row[\"COFRS REPORTING CATEGORY\"] = safe_search(r\"COFRS REPORTING CATEGORY:(\\S+)\", l4)\n",
    "    row[\"FACS REF NO\"] = safe_search(r\"FACS REF NO:(\\S*)\", l4)\n",
    "    \n",
    "    l5 = lines[5] if len(lines)>5 else \"\"\n",
    "    tc = safe_search(r\"TIME CHARGED:\\s*([\\d,]+\\.?\\d*)\\s*DAYS\", l5)\n",
    "    row[\"TIME CHARGED\"] = f\"{tc} DAYS\" if tc else \"\"\n",
    "    row[\"TIME ALLOW\"]   = safe_search(r\"TIME ALLOW:\\s*([\\d,]+\\.?\\d* WORK DAYS)\", l5)\n",
    "    \n",
    "    l6 = lines[6] if len(lines)>6 else \"\"\n",
    "    row[\"CONTRACT DESCRIPTION\"] = l6.split(\"PERCENT TIME\")[0].replace(\"CONTRACT DESCRIPTION:\",\"\").strip()\n",
    "    row[\"PERCENT TIME\"]       = safe_search(r\"PERCENT TIME\\s*[:]\\s*([\\d,]+\\.?\\d*%?)\", l6)\n",
    "    row[\"ORIG TIME ALLOW\"]    = safe_search(r\"ORIG TIME ALLOW\\s*[:]\\s*([\\d,]+\\.?\\d* WORK DAYS)\", l6)\n",
    "    \n",
    "    l7 = lines[7] if len(lines)>7 else \"\"\n",
    "    row[\"PROJECT NO\"]     = safe_search(r\"PROJECT NO:\\s*(.+)\", l7).strip()\n",
    "    \n",
    "    l9 = lines[9] if len(lines)>9 else \"\"\n",
    "    row[\"NAME OF ROAD\"]   = safe_search(r\"NAME OF ROAD:\\s*(.+)\", l9).strip()\n",
    "    \n",
    "    l10 = lines[10] if len(lines)>10 else \"\"\n",
    "    row[\"PROJECT COUNTIES\"] = safe_search(r\"PROJECT COUNTIES:\\s*(.+)\", l10).strip()\n",
    "    \n",
    "    l12 = lines[12] if len(lines)>12 else \"\"\n",
    "    row[\"PAY PERIOD ENDING\"]    = safe_search(r\"PAY PERIOD ENDING\\s+(\\S+)\", l12)\n",
    "    row[\"DATE TIME STARTED\"]    = safe_search(r\"DATE TIME STARTED\\s+(\\S+)\", l12)\n",
    "    \n",
    "    l13 = lines[13] if len(lines)>13 else \"\"\n",
    "    row[\"DATE LET\"]         = safe_search(r\"DATE LET\\s+(\\S+)\", l13)\n",
    "    row[\"DATE WORK BEGAN\"]  = safe_search(r\"DATE WORK BEGAN\\s+(\\S+)\", l13)\n",
    "    \n",
    "    l14 = lines[14] if len(lines)>14 else \"\"\n",
    "    row[\"DATE AWARDED\"]      = safe_search(r\"DATE AWARDED\\s+(\\S+)\", l14)\n",
    "    row[\"DATE TIME STOPPED\"] = safe_search(r\"DATE TIME STOPPED\\s+(\\S+)\", l14)\n",
    "    \n",
    "    l15 = lines[15] if len(lines)>15 else \"\"\n",
    "    row[\"DATE CONTRACT EXECUTED\"] = safe_search(r\"DATE CONTRACT EXECUTED\\s+(\\S+)\", l15)\n",
    "    row[\"DATE ACCEPTED\"]         = safe_search(r\"DATE ACCEPTED\\s+(\\S+)\", l15)\n",
    "    \n",
    "    l16 = lines[16] if len(lines)>16 else \"\"\n",
    "    row[\"DATE NOTICE TO PROCEED\"] = safe_search(r\"DATE NOTICE TO PROCEED\\s+(\\S+)\", l16)\n",
    "    \n",
    "    # --- AMOUNTS & BRACKETS ---\n",
    "    l18 = lines[18] if len(lines)>18 else \"\"\n",
    "    row[\"CURRENT PROJECT AMT\"] = safe_search(r\"CURRENT PROJECT AMT:\\s*\\$?\\s*([\\d,]+\\.\\d+)\", l18)\n",
    "    part = re.search(r\"PARTICIPATING\\s*\\$?\\s*([\\d,]+\\.\\d+)\\s*\\$?\\s*(-?[\\d,]+\\.\\d+)\", l18)\n",
    "    if part:\n",
    "        row[\"PARTICIPATING_CURRENT_TOTAL\"], row[\"PARTICIPATING_THIS_ESTIMATE\"] = part.group(1), part.group(2)\n",
    "    \n",
    "    l19 = lines[19] if len(lines)>19 else \"\"\n",
    "    row[\"AWARD PROJECT AMT\"] = safe_search(r\"AWARD PROJECT AMT:\\s*\\$?\\s*([\\d,]+\\.\\d+)\", l19)\n",
    "    nonp = re.search(r\"NON-PARTICIPATING\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l19)\n",
    "    if nonp:\n",
    "        row[\"NON-PARTICIPATING_CURRENT_TOTAL\"], row[\"NON-PARTICIPATING_THIS_ESTIMATE\"] = nonp.group(1), nonp.group(2)\n",
    "    \n",
    "    l20 = lines[20] if len(lines)>20 else \"\"\n",
    "    pc = safe_search(r\"PERCENT COMPLETE:\\s*([\\d,]+\\.\\d+)%?\", l20)\n",
    "    row[\"PERCENT COMPLETE\"] = f\"{pc}%\" if pc and not pc.endswith('%') else pc\n",
    "    te = re.search(r\"TOTAL EARNINGS\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l20)\n",
    "    if te:\n",
    "        row[\"TOTAL EARNINGS_CURRENT_TOTAL\"], row[\"TOTAL EARNINGS_THIS_ESTIMATE\"] = te.group(1), te.group(2)\n",
    "    \n",
    "    l21 = lines[21] if len(lines)>21 else \"\"\n",
    "    row[\"FUNDS AVAILABLE\"] = safe_search(r\"FUNDS AVAILABLE:\\s*\\$?\\s*([\\d,]+\\.\\d+)\", l21)\n",
    "    sm = re.search(r\"STOCKPILED MATERIALS\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l21)\n",
    "    if sm:\n",
    "        row[\"STOCKPILED MATERIALS_CURRENT_TOTAL\"], row[\"STOCKPILED MATERIALS_THIS_ESTIMATE\"] = sm.group(1), sm.group(2)\n",
    "    \n",
    "    l22 = lines[22] if len(lines)>22 else \"\"\n",
    "    ge = re.search(r\"GROSS EARNINGS\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l22)\n",
    "    if ge:\n",
    "        row[\"GROSS EARNINGS_CURRENT_TOTAL\"], row[\"GROSS EARNINGS_THIS_ESTIMATE\"] = ge.group(1), ge.group(2)\n",
    "    \n",
    "    l23 = lines[23] if len(lines)>23 else \"\"\n",
    "    row[\"TOTAL CLAIMS\"] = safe_search(r\"TOTAL CLAIMS:\\s*\\$?\\s*([\\d,]+\\.\\d+)\", l23)\n",
    "    rt = re.search(r\"RETAINAGE\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l23)\n",
    "    if rt:\n",
    "        row[\"RETAINAGE_CURRENT_TOTAL\"], row[\"RETAINAGE_THIS_ESTIMATE\"] = rt.group(1), rt.group(2)\n",
    "    \n",
    "    l24 = lines[24] if len(lines)>24 else \"\"\n",
    "    se = re.search(r\"SECURITIES ENCUMBERED\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l24)\n",
    "    if se:\n",
    "        row[\"SECURITIES ENCUMBERED_CURRENT_TOTAL\"], row[\"SECURITIES ENCUMBERED_THIS_ESTIMATE\"] = se.group(1), se.group(2)\n",
    "    \n",
    "    l25 = lines[25] if len(lines)>25 else \"\"\n",
    "    ne = re.search(r\"NET EARNINGS\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l25)\n",
    "    if ne:\n",
    "        row[\"NET EARNINGS_CURRENT_TOTAL\"], row[\"NET EARNINGS_THIS_ESTIMATE\"] = ne.group(1), ne.group(2)\n",
    "    \n",
    "    l26 = lines[26] if len(lines)>26 else \"\"\n",
    "    ld = re.search(r\"LIQUIDATED DAMAGES\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l26)\n",
    "    if ld:\n",
    "        row[\"LIQUIDATED DAMAGES_CURRENT_TOTAL\"], row[\"LIQUIDATED DAMAGES_THIS_ESTIMATE\"] = ld.group(1), ld.group(2)\n",
    "    \n",
    "    l27 = lines[27] if len(lines)>27 else \"\"\n",
    "    aa = re.search(r\"AUTOPAY ADJUSTMENT\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l27)\n",
    "    if aa:\n",
    "        row[\"AUTOPAY ADJUSTMENT_CURRENT_TOTAL\"], row[\"AUTOPAY ADJUSTMENT_THIS_ESTIMATE\"] = aa.group(1), aa.group(2)\n",
    "    \n",
    "    l28 = lines[28] if len(lines)>28 else \"\"\n",
    "    ad = re.search(r\"AMOUNT DUE\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l28)\n",
    "    if ad:\n",
    "        row[\"AMOUNT DUE_CURRENT_TOTAL\"], row[\"AMOUNT DUE_THIS_ESTIMATE\"] = ad.group(1), ad.group(2)\n",
    "    \n",
    "    l29 = lines[29] if len(lines)>29 else \"\"\n",
    "    oa = re.search(r\"OTHER ADJUSTMENTS\\s+(-?[\\d,]+\\.\\d+)\\s+(-?[\\d,]+\\.\\d+)\", l29)\n",
    "    if oa:\n",
    "        row[\"OTHER ADJUSTMENTS_CURRENT_TOTAL\"], row[\"OTHER ADJUSTMENTS_THIS_ESTIMATE\"] = oa.group(1), oa.group(2)\n",
    "    \n",
    "    # PAYMENT DUE \n",
    "    l30 = lines[30] if len(lines) > 30 else \"\"\n",
    "    row[\"PAYMENT DUE\"] = safe_search(r\"PAYMENT DUE\\s+\\$?\\s*(-?[\\d,]+\\.\\d+)\", l30)\n",
    "    \n",
    "    l31 = lines[31] if len(lines) > 31 else \"\"\n",
    "    row[\"APPROVED FOR PAYMENT BY\"] = safe_search(r\"APPROVED FOR PAYMENT BY(\\S*)\", l31)\n",
    "\n",
    "    l32 = lines[32] if len(lines) > 32 else \"\"\n",
    "    row[\"PROJECT COMMENT\"] = safe_search(r\"PROJECT COMMENT:(\\S*)\", l32)\n",
    "\n",
    "    l33 = lines[33] if len(lines) > 33 else \"\"\n",
    "    row[\"ESTIMATE COMMENT\"] = safe_search(r\"ESTIMATE COMMENT:(\\S*)\", l33)\n",
    "\n",
    "\n",
    "    return pd.DataFrame([row], columns=ALL_COLUMNS)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    file_page1 = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Colorado\\Example_11582_27.pdf\"\n",
    "    df1 = extract_page1(file_page1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c4ce51",
   "metadata": {},
   "source": [
    "WV Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import schedule\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "def west_virginia_scraping():\n",
    "\n",
    "    # Configuração de Logging\n",
    "    logging.basicConfig(\n",
    "        filename='west_virginia_scraping.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s:%(levelname)s:%(message)s',\n",
    "        # handlers=[\n",
    "        # # logging.FileHandler(r'C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\West_virginia\\west_virginia_scraping.log'),  # Log to file\n",
    "        # logging.StreamHandler() # Log to console\n",
    "        # ]\n",
    "    )\n",
    "    # Email configuration\n",
    "    SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "    SMTP_PORT = 587  # Usually 587 for TLS\n",
    "    EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "    EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "    EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "    def send_email(subject, body):\n",
    "        try:\n",
    "            msg = MIMEMultipart()\n",
    "            msg['From'] = EMAIL_SENDER\n",
    "            msg['To'] = EMAIL_RECIPIENT\n",
    "            msg['Subject'] = subject\n",
    "            msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "            with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "                server.starttls()\n",
    "                server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "                server.send_message(msg)\n",
    "            print(\"Email notification sent successfully.\")\n",
    "            logging.info(\"Email notification sent successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to send email notification: {e}\")\n",
    "            logging.error(f\"Failed to send email notification: {e}\")\n",
    "\n",
    "    # Automatic driver installer\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    url = 'https://www.wva.state.wv.us/wvdot/surety/'\n",
    "\n",
    "    # Inicia uma nova sessão do navegador\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.get(url)\n",
    "    row_data_list_wv = []\n",
    "    header_data_wv = []\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.ID, 'ddlVendors'))\n",
    "        )\n",
    "        vendor_dropdown = driver.find_element(By.ID, 'ddlVendors')\n",
    "        time.sleep(3)\n",
    "        select = Select(vendor_dropdown)\n",
    "\n",
    "        for i in range(len(select.options)):\n",
    "            vendor_dropdown = driver.find_element(By.ID, 'ddlVendors')\n",
    "            select = Select(vendor_dropdown)\n",
    "            vendor_name = select.options[i].get_attribute(\n",
    "                \"textContent\").strip()\n",
    "            if i == 0:\n",
    "                continue  # Skip the 'select vendor' option\n",
    "\n",
    "            logging.info(f\"Scraping for vendor: {vendor_name}\")\n",
    "            select.select_by_visible_text(vendor_name)\n",
    "\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located(\n",
    "                    (By.XPATH, '//table[@ID=\"gvContracts\"]/tbody/tr'))\n",
    "            )\n",
    "\n",
    "            if i == 1 or not header_data_wv:\n",
    "                table_header = driver.find_elements(\n",
    "                    By.XPATH, '//table[@ID=\"gvContracts\"]/tbody/tr/th')\n",
    "                header_data_wv.extend([header.get_attribute('textContent').strip(\n",
    "                ) for header in table_header if header.get_attribute('textContent').strip()])\n",
    "                header_data_wv = ['Contract_Vendors'] + header_data_wv\n",
    "\n",
    "            table_data = driver.find_elements(\n",
    "                By.XPATH, '//table[@ID=\"gvContracts\"]/tbody/tr')\n",
    "            for j, row in enumerate(table_data):\n",
    "                if j > 0:\n",
    "                    cells = row.find_elements(By.XPATH, './td')\n",
    "                    current_row_data = [vendor_name]\n",
    "                    current_row_data.extend(\n",
    "                        [cell.text for cell in cells if cell.text])\n",
    "                    row_data_list_wv.append(current_row_data)\n",
    "\n",
    "    except TimeoutException as e:\n",
    "        logging.error(f\"Timeout Exception: {e}\")\n",
    "    finally:\n",
    "        # Fechar o navegador\n",
    "        driver.quit()\n",
    "\n",
    "    # Create a Dataframe\n",
    "    WV_DOT_Data = pd.DataFrame(data=row_data_list_wv, columns=header_data_wv)\n",
    "\n",
    "    # POST PROCESSING\n",
    "    df = WV_DOT_Data.copy()\n",
    "    df.rename(columns = {'Federal/State Project Number':'Project_Number','Description':'Project_Description','Contract Amount':'Project_Cost_Total','Plan Completion Date':'Project_Comp_Est','Substantial Completion Date':'Project_Comp_Substantial',\n",
    "            'Complete Date':'Project_Comp_Final', 'Release Date': 'Project_Close','Contract_Vendors':'Contractor_Name','Paid Amount':'Payment_Amount_Total','Percent Complete':'Payment_Total_Percent'}, inplace=True)\n",
    "    def parse_money(value):\n",
    "        # Remove dollar signs and commas\n",
    "        value = value.replace('$', '').replace(',', '')\n",
    "        # Convert values in parentheses to negative numbers\n",
    "        if '(' in value and ')' in value:\n",
    "            value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "        return float(value)\n",
    "    df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "    df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "    df['Payment_Balance'] = df['Project_Cost_Total'] - df['Payment_Amount_Total']\n",
    "    df['Project_Comp_Est'] = pd.to_datetime(df['Project_Comp_Est'],errors='coerce')\n",
    "    df['Project_Comp_Substantial'] = pd.to_datetime(df['Project_Comp_Substantial'],errors='coerce')\n",
    "    df['Project_Comp_Final'] = pd.to_datetime(df['Project_Comp_Final'],errors='coerce')\n",
    "    df['Project_Close'] = pd.to_datetime(df['Project_Close'],errors='coerce')\n",
    "\n",
    "    \n",
    "    df['Project_Comp_Est'] = df['Project_Comp_Est'].dt.strftime('%m/%d/%Y')\n",
    "    df['Project_Comp_Substantial'] = df['Project_Comp_Substantial'].dt.strftime('%m/%d/%Y')\n",
    "    df['Project_Comp_Final'] = df['Project_Comp_Final'].dt.strftime('%m/%d/%Y')\n",
    "    df['Project_Close'] = df['Project_Close'].dt.strftime('%m/%d/%Y')\n",
    "    \n",
    "    EST = pytz.timezone('US/Eastern')\n",
    "    now = datetime.now(EST)\n",
    "    current_date = now.strftime(\"%m/%d/%Y\")\n",
    "    df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "    # DUCKDB INTEGRATION\n",
    "    # File to store DuckDB data\n",
    "    db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\West_virginia\\data_store_WV.duckdb\"\n",
    "    table_name = \"WV_DOT\"\n",
    "    \n",
    "    # Current scraped data\n",
    "    scraped_data = df\n",
    "\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(db_file)\n",
    "\n",
    "    # Create table if not exists\n",
    "    con.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    Contractor_Name  TEXT,\n",
    "    Project_Number  TEXT,\n",
    "    Payment_Number  INTEGER,\n",
    "    Project_Description\tTEXT,\n",
    "    Project_Comp_Est  TEXT,\n",
    "    Project_Comp_Substantial  TEXT,\t\n",
    "    Project_Comp_Final  TEXT,\t\n",
    "    Project_Close  TEXT,\n",
    "    Payment_Amount DOUBLE,\t\n",
    "    Previous_Payment_Amount_Total DOUBLE,\n",
    "    Project_Cost_Total  DOUBLE,\t\n",
    "    Payment_Amount_Total  DOUBLE,\n",
    "    Payment_Total_Percent  TEXT,\n",
    "    Payment_Balance  DOUBLE,\n",
    "    Pull_Date_Initial TEXT,\n",
    "    Payment_Amount_Percent FLOAT,\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or Update Logic\n",
    "    # Load existing data from DuckDB\n",
    "    existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "    # Deduplicate and merge\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "        # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "        combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial', 'Previous_Payment_Amount_Total', 'Payment_Number', 'Payment_Amount_Percent', 'Payment_Amount'])].columns,keep=\"first\") \n",
    "        # Post processing before loading into duckdb\n",
    "        combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "        combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "        # Group by Project_Number and calculate the previous Payment_Amount_Total\n",
    "        combined_data['Previous_Payment_Amount_Total'] = combined_data.groupby(\"Project_Number\")[\"Payment_Amount_Total\"].shift(-1)\n",
    "        # Assign a reversed payment estimate number\n",
    "        combined_data[\"Payment_Number\"] = combined_data.groupby(\"Project_Number\").cumcount(ascending=False)\n",
    "        # Calculate the Current Amount Paid\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount_Total'] - combined_data['Previous_Payment_Amount_Total']\n",
    "        # Fill NaN for the first record in each group (no previous record exists)\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount'].fillna(combined_data['Payment_Amount_Total'])\n",
    "        # Revert the formatting of pull_date_initial column\n",
    "        combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "        # Calculate Payment Amount Percent\n",
    "        combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "        table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "        correct_order = table_info['column_name'].tolist()\n",
    "        # Reorder the DataFrame to avoid conversion errors\n",
    "        combined_data = combined_data[correct_order]\n",
    "\n",
    "    else:\n",
    "        combined_data = scraped_data\n",
    "        # Post processing before loading into duckdb\n",
    "        combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "        combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "        # Group by Project_Number and calculate the previous Payment_Amount_Total\n",
    "        combined_data['Previous_Payment_Amount_Total'] = combined_data.groupby(\"Project_Number\")[\"Payment_Amount_Total\"].shift(-1)\n",
    "        # Assign a reversed payment estimate number\n",
    "        combined_data[\"Payment_Number\"] = combined_data.groupby(\"Project_Number\").cumcount(ascending=False)\n",
    "        # Calculate the Current Amount Paid\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount_Total'] - combined_data['Previous_Payment_Amount_Total']\n",
    "        # Fill NaN for the first record in each group (no previous record exists)\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount'].fillna(combined_data['Payment_Amount_Total'])\n",
    "        # Revert the formatting of pull_date_initial column\n",
    "        combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "        # Calculate Payment Amount Percent\n",
    "        combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "        table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "        correct_order = table_info['column_name'].tolist()\n",
    "        # Reorder the DataFrame to avoid conversion errors\n",
    "        combined_data = combined_data[correct_order]\n",
    "\n",
    "    # Replace the table with the updated data\n",
    "    print(combined_data)\n",
    "    con.execute(f\"DELETE FROM {table_name}\")\n",
    "    con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "    # Close connection\n",
    "    con.close()\n",
    "    send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "    print(\"West Virginia scraping completed and DUCKDB file updated Successfully.\")\n",
    "    logging.info(\n",
    "        'West Virginia scraping completed and DUCKDB file updated Successfully.')\n",
    "west_virginia_scraping()\n",
    "# # Function to check the date\n",
    "# def check_date():\n",
    "#     EST = pytz.timezone('US/Eastern')\n",
    "#     now = datetime.now(EST)\n",
    "#     if now.day in [2, 3]:\n",
    "#         print(f\"Today is {now.strftime('%B %d')}. Starting time checks...\")\n",
    "#         logging.info(f\"Today is {now.strftime('%B %d')}. Starting time checks...\")\n",
    "#         check_time_till_execution()\n",
    "\n",
    "# # Function to check the time every minute on the due date\n",
    "# def check_time_till_execution():\n",
    "#     EST = pytz.timezone('US/Eastern')\n",
    "#     end_time = datetime.now(EST).replace(hour=23, minute=59, second=59)  # End of the day\n",
    "#     while datetime.now(EST) < end_time:\n",
    "#         now = datetime.now(EST)\n",
    "#         if now.hour == 19 and now.minute == 48:\n",
    "#             print(\"Scheduler started. Waiting for scheduled jobs...\")\n",
    "#             west_virginia_scraping()\n",
    "#             break\n",
    "#         time.sleep(60)  # Check every minute\n",
    "#     print(\"Finished time checks for the day.\")\n",
    "#     logging.info(\"Finished time checks for the day.\")\n",
    "\n",
    "# # Schedule the daily date check\n",
    "# schedule.every().day.at('19:47').do(check_date) \n",
    "# logging.info(\"Scheduler started. Waiting for scheduled jobs...\")\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981555ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"West_Virginia\", \"data_store_WV\", \"WV_DOT\")\n",
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\West_virginia\\Monthly\\wv_bulk_pipeline_april_2025_updated.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a3e26",
   "metadata": {},
   "source": [
    "NY City Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6acf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def ny_city_scraping():\n",
    "    # Set download directory\n",
    "    download_dir = r\"C:\\Users\\TarunPongulaty\\Downloads\"  # Replace with desired path\n",
    "\n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": download_dir,  # Set default download directory\n",
    "        \"download.prompt_for_download\": False,  # Disable download prompts\n",
    "        \"directory_upgrade\": True,  # Automatically overwrite files\n",
    "        \"safebrowsing.enabled\": True  # Enable safe browsing\n",
    "    })\n",
    "    # Automatic driver installer\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    # URL of the site\n",
    "    url = 'https://wwe2.osc.state.ny.us/transparency/contracts/contractsearch.cfm'\n",
    "     # Email configuration\n",
    "    SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "    SMTP_PORT = 587  # Usually 587 for TLS\n",
    "    EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "    EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "    EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "    def send_email(subject, body):\n",
    "            try:\n",
    "                msg = MIMEMultipart()\n",
    "                msg['From'] = EMAIL_SENDER\n",
    "                msg['To'] = EMAIL_RECIPIENT\n",
    "                msg['Subject'] = subject\n",
    "                msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "                with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "                    server.starttls()\n",
    "                    server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "                    server.send_message(msg)\n",
    "                print(\"Email notification sent successfully.\")\n",
    "                logging.info(\"Email notification sent successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to send email notification: {e}\")\n",
    "                logging.error(f\"Failed to send email notification: {e}\")\n",
    "    # Retreive list of all the state agencies\n",
    "    # Replace with the correct path to your chromedriver\n",
    "    def get_agency():\n",
    "        # Start a new browser session\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            # wait for agency option to appear\n",
    "            agency_name = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/div[2]/div/form/div[1]/p[1]/span/span[1]/span/ul/li/input\"))\n",
    "            )\n",
    "            agency_name.click()\n",
    "            agency_name_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/div[2]/div/form/div[1]/p[1]/select\"))\n",
    "            )\n",
    "            select = Select(agency_name_dropdown)\n",
    "            agency_name_list = [name.get_attribute(\"textContent\").strip() for name in select.options if name.get_attribute(\"textContent\").strip()]  \n",
    "            # delete \"all state agencies\" value\n",
    "            del agency_name_list[0]\n",
    "            agency_data = pd.DataFrame(data = agency_name_list, columns = [\"Agency_Name\"])\n",
    "\n",
    "        finally:\n",
    "            # Close the browser\n",
    "            driver.quit()\n",
    "        return agency_data\n",
    "\n",
    "    def get_bulk_file():\n",
    "        try:\n",
    "            # Start a new browser session\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            def is_download_complete(file_path):\n",
    "                \"\"\"\n",
    "                Check if the file is complete by:\n",
    "                1. Ensuring it's not a .crdownload file (Chrome's in-progress download extension).\n",
    "                2. Ensuring it has a non-zero size.\n",
    "                \"\"\"\n",
    "                return os.path.exists(file_path) and not file_path.endswith(\".crdownload\") and os.path.getsize(file_path) > 0\n",
    "\n",
    "            def wait_for_download(download_dir, timeout=30):\n",
    "                \"\"\"\n",
    "                Wait for the most recently downloaded file to appear in the download directory.\n",
    "                Ensures the file is fully downloaded before returning its path.\n",
    "                :param download_dir: The directory to monitor for downloads.\n",
    "                :param timeout: Maximum time to wait for a file (in seconds).\n",
    "                :return: The full path of the downloaded file.\n",
    "                \"\"\"\n",
    "                start_time = time.time()\n",
    "                while True:\n",
    "                    # List all files in the download directory\n",
    "                    files = os.listdir(download_dir)\n",
    "\n",
    "                    if files:\n",
    "                        # Identify the most recently created file in the directory\n",
    "                        latest_file = max(files, key=lambda f: os.path.getctime(os.path.join(download_dir, f)))\n",
    "                        latest_file_path = os.path.join(download_dir, latest_file)\n",
    "\n",
    "                        # Check if the download is complete\n",
    "                        if is_download_complete(latest_file_path):\n",
    "                            return latest_file_path\n",
    "\n",
    "                    # Break the loop if timeout is exceeded\n",
    "                    if time.time() - start_time > timeout:\n",
    "                        raise TimeoutError(\"Download did not complete within the timeout period.\")\n",
    "                    \n",
    "                    time.sleep(1)  # Wait for a short time before checking again\n",
    "\n",
    "            driver.get(url)\n",
    "            # wait for agency option to appear\n",
    "            agency_name = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/div[2]/div/form/div[1]/p[1]/span/span[1]/span/ul/li/input\"))\n",
    "            )\n",
    "            agency_name.click()\n",
    "            # select \"all state agencies\"\n",
    "            all_state_agencies = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/div[2]/div/form/div[1]/p[1]/select/option[1]\"))\n",
    "            )\n",
    "            all_state_agencies.click()\n",
    "            search = driver.find_element(By.ID, 'b')\n",
    "            # Find the contract number input box and enter the contract number\n",
    "            search.click()\n",
    "            # find and download the final report\n",
    "            download_report = WebDriverWait(driver, 25).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, 'Download Summary Contract Information to an Excel Spreadsheet'))\n",
    "            )\n",
    "            download_report.click()\n",
    "            print(\"Waiting for contract data download to complete...\")\n",
    "            # wait until download is completed\n",
    "            time.sleep(70)\n",
    "            try:\n",
    "                downloaded_file = wait_for_download(download_dir, timeout=60)\n",
    "                print(f\"File successfully downloaded at: {downloaded_file}\")\n",
    "                ny_bulk = pd.read_excel(downloaded_file,skiprows=7)\n",
    "            except TimeoutError as e:\n",
    "                print(e)\n",
    "            download_report_amendment = WebDriverWait(driver, 25).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, 'Download Additional Contract and Related Amendment Data for OSC approved transactions'))\n",
    "            )\n",
    "            download_report_amendment.click()\n",
    "            print(\"Waiting for amendment contract data download to complete...\")\n",
    "            # wait until download is completed\n",
    "            time.sleep(120)\n",
    "            try:\n",
    "                downloaded_file = wait_for_download(download_dir, timeout=60)\n",
    "                print(f\"File successfully downloaded at: {downloaded_file}\")\n",
    "                ny_amendment = pd.read_csv(downloaded_file,skiprows=1)\n",
    "            except TimeoutError as e:\n",
    "                print(e)\n",
    "            \n",
    "        finally:\n",
    "            # Close the browser\n",
    "            driver.quit()\n",
    "        return ny_bulk, ny_amendment\n",
    "    \n",
    "    def append_lookup(unique_departments, agency_data, df_lookup):\n",
    "        unique_departments = ny_bulk['DEPARTMENT/FACILITY'].unique()\n",
    "        unique_departments = pd.DataFrame(data=unique_departments,columns=[\"DEPARTMENT/FACILITY\"])\n",
    "        \n",
    "        client = OpenAI(api_key=os.getenv(\"OPEN_API_KEY\"))\n",
    "        agency_data = agency_data\n",
    "        departments = unique_departments.copy()\n",
    "        results_df = pd.DataFrame()\n",
    "        # unique_departments.iloc[i:i+batch_size]\n",
    "        def get_completion_from_messages(messages, model=\"gpt-4-turbo\", temperature=0):\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,         \n",
    "                temperature=temperature, # this is the degree of randomness of the model's output\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        batch_size = 5\n",
    "        for i in range(0, len(departments), batch_size):\n",
    "            prompt = f\"\"\"\n",
    "            You are a professional analyst in the department of transportation domain for new york city, an automated service and you should be able to map the department/facility name to its respective Agency. Below are the columns from the two different tables.\n",
    "            1. 'DEPARTMENT/FACILITY' from 'departments' table\n",
    "            2. 'Agency_Name' from 'agency_data' table\n",
    "            To give you more context, a common trend you'd see is that the names of 'Department/Facility' are often similar to their respective 'Agency_Name'. However, in some cases, this is very different and not obvious. So, in such cases,\n",
    "            please browse the web to find relevant information which can help you map the data. One of the good resources is the following site 'https://wwe2.osc.state.ny.us/TRANSPARENCY/CONTRACTS/CONTRACTSEARCH.CFM'. You can browse other web sources as well to find the relevent information.\n",
    "            Also, each 'Agency_Name' can have one ore more departments or facilities. So, it is a one to many relationship.\n",
    "            The following are the 2 tables: {unique_departments.iloc[i:i+batch_size],agency_data}\n",
    "            \"\"\"\n",
    "\n",
    "            context = [ {'role':'system', 'content':prompt} ]# accumulate messages\n",
    "\n",
    "            messages =  context.copy()\n",
    "            messages.append(\n",
    "            {'role':'system', 'content':'''The consolidated table after mapping 'DEPARTMENT/FACILITY' with 'Agency_Name' should exactly be in the form of a json with columns/keys: \"Agency_Name\", \"DEPARTMENT/FACILITY\".\\\n",
    "            Also I just want the json output so I'll be able to convert it to a dataframe. Do not add any fillers/words like 'json' around the output.\n",
    "            Do not skip anything and bypass the token limit if needed'''}) \n",
    "\n",
    "            response = get_completion_from_messages(messages, temperature=0)\n",
    "            # print(response)\n",
    "            try:\n",
    "                df_response = pd.read_json(response) \n",
    "                results_df = pd.concat([results_df,df_response],ignore_index=True)\n",
    "            except ValueError:\n",
    "                print(response)\n",
    "                print(\"ERROR AT BATCH ITERATION NUMBER:\", i)\n",
    "                continue \n",
    "        # Append changes back to original lookup table\n",
    "        results_df = pd.concat([df_lookup,results_df],ignore_index=True)\n",
    "        # Write the updated data to a new lookup table for further review\n",
    "        results_df.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\Monthly\\agency_lookup_table_ny_1.xlsx\")\n",
    "    agency_data = get_agency()\n",
    "    # Read the latest download bulk files.\n",
    "    ny_bulk, ny_amendment = get_bulk_file()\n",
    "    # Read the lookup table\n",
    "    df_lookup = pd.read_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\agency_lookup_table_ny.xlsx\")\n",
    "    # Check for changes in departments and Agency names\n",
    "    unique_departments = ny_bulk['DEPARTMENT/FACILITY'].unique()\n",
    "    unique_departments = pd.DataFrame(data=unique_departments,columns=[\"DEPARTMENT/FACILITY\"])\n",
    "    previous_unique_departments = df_lookup['DEPARTMENT/FACILITY'].unique()\n",
    "    previous_unique_departments = pd.DataFrame(data=previous_unique_departments,columns=[\"DEPARTMENT/FACILITY\"])\n",
    "\n",
    "    departments_join = pd.merge(unique_departments,previous_unique_departments, left_on='DEPARTMENT/FACILITY', right_on = 'DEPARTMENT/FACILITY', how = 'outer', indicator=True)\n",
    "    departments_join_left_only = departments_join[departments_join['_merge'] == 'left_only']\n",
    "    previous_unique_agencies = df_lookup['Agency_Name'].unique()\n",
    "    previous_unique_agencies = pd.DataFrame(data=previous_unique_agencies,columns=[\"Agency_Name\"])\n",
    "    df_lookup_join = pd.merge(agency_data,previous_unique_agencies, left_on='Agency_Name', right_on = 'Agency_Name', how = 'outer', indicator=True )\n",
    "    df_lookup_join_left_only = df_lookup_join[df_lookup_join['_merge'] == 'left_only']\n",
    "\n",
    "    if not departments_join_left_only.empty or not df_lookup_join_left_only.empty:\n",
    "        send_email('Agency/department Data Change', 'Check if updated Agency/Department data has been written to a new lookup table and review the changes and update the original lookup table')\n",
    "        append_lookup(departments_join_left_only, agency_data, df_lookup)\n",
    "        # Reading this new data without reviewing is risky. After review, new agency/department information can be appended easily. To review, compare the new table with the original one to look at differences.\n",
    "        # df_lookup = pd.read_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\Monthly\\agency_lookup_table_ny_1.xlsx\") \n",
    "\n",
    "    # POST PROCESSING\n",
    "    # Split into contract type and contract sub type\n",
    "    ny_bulk[['Contract_Type', 'Contract_Subtype']] = ny_bulk['CONTRACT TYPE'].str.split(' - ', n=1, expand = True)\n",
    "    del ny_bulk['CONTRACT TYPE'] # use this option if it needs to be deleted\n",
    "    ny_dot_data = pd.merge(ny_bulk,df_lookup, left_on=\"DEPARTMENT/FACILITY\", right_on=\"DEPARTMENT/FACILITY\", how=\"left\")\n",
    "    df = ny_dot_data.copy()\n",
    "    \n",
    "\n",
    "    df = ny_dot_data.copy()\n",
    "    df.rename(columns = {'VENDOR NAME':'Contractor_Name','DEPARTMENT/FACILITY': 'Project_Dept_Facility','CONTRACT NUMBER':'Contract_Number','CURRENT CONTRACT AMOUNT':'Project_Cost_Total',\n",
    "                        'SPENDING TO DATE': 'Payment_Amount_Total',\t'CONTRACT START DATE':'Project_Start_Actual','CONTRACT END DATE':'Project_Comp_Est','CONTRACT DESCRIPTION':'Project_Description',\n",
    "                        'ORIGINAL CONTRACT APPROVED/FILED DATE':'Project_Bid_Awarded'}, inplace=True)\n",
    "    def parse_money(value):\n",
    "        # Check if the value is already a float or None/NaN\n",
    "        if pd.isna(value) or isinstance(value, (float, int)):\n",
    "            return value\n",
    "        # Remove dollar signs and commas\n",
    "        value = value.replace('$', '').replace(',', '')\n",
    "        # Convert values in parentheses to negative numbers\n",
    "        if '(' in value and ')' in value:\n",
    "            value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "        return float(value)\n",
    "    df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "    df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "    df['Payment_Balance'] = df['Project_Cost_Total'] - df['Payment_Amount_Total']\n",
    "\n",
    "    EST = pytz.timezone('US/Eastern')\n",
    "    now = datetime.now(EST)\n",
    "    current_date = now.strftime(\"%m/%d/%Y\")\n",
    "    df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "    # DUCKDB INTEGRATION\n",
    "    # File to store DuckDB data\n",
    "    db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\data_store_NY.duckdb\"\n",
    "    table_name = \"NY_DOT\"\n",
    "\n",
    "    # Current scraped data\n",
    "    scraped_data = df\n",
    "\n",
    "    # Connect to DuckDB\n",
    "    con = duckdb.connect(db_file)\n",
    "\n",
    "    # Create table if not exists\n",
    "    con.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "    Contractor_Name  TEXT,\n",
    "    Project_Dept_Facility TEXT,\n",
    "    Agency_Name  TEXT,\n",
    "    Contract_Number  TEXT,\n",
    "    Contract_Type  TEXT,\n",
    "    Contract_Subtype TEXT,\n",
    "    Payment_Number  INTEGER,\n",
    "    Project_Description\tTEXT,\n",
    "    Project_Start_Actual  TEXT,\n",
    "    Project_Comp_Est  TEXT,\t\n",
    "    Project_Bid_Awarded  TEXT,\n",
    "    Payment_Amount DOUBLE,\t\n",
    "    Previous_Payment_Amount_Total DOUBLE,\n",
    "    Project_Cost_Total  DOUBLE,\t\n",
    "    Payment_Amount_Total  DOUBLE,\n",
    "    Payment_Total_Percent  TEXT,\n",
    "    Payment_Balance  DOUBLE,\n",
    "    Pull_Date_Initial TEXT,\n",
    "    Payment_Amount_Percent FLOAT,\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert or Update Logic\n",
    "    # Load existing data from DuckDB\n",
    "    existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "    # Deduplicate and merge\n",
    "    if not existing_data.empty:\n",
    "        combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "        # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "        combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial','CONTRACT TYPE', 'Contract_Subtype','Payment_Total_Percent','Previous_Payment_Amount_Total', 'Payment_Number', 'Payment_Amount_Percent', 'Payment_Amount'])].columns,keep=\"first\") \n",
    "        # Post processing before loading into duckdb\n",
    "        combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "        combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "        # Group by Contract_Number, Contractor_Name and Project_Description (Since Ny has different jobs within the same contract number) and calculate the previous Payment_Amount_Total\n",
    "        combined_data['Previous_Payment_Amount_Total'] = combined_data.groupby([\"Contract_Number\",\"Contractor_Name\",\"Project_Description\"])[\"Payment_Amount_Total\"].shift(-1)\n",
    "        # Assign a reversed payment estimate number\n",
    "        combined_data[\"Payment_Number\"] = combined_data.groupby([\"Contract_Number\",\"Contractor_Name\",\"Project_Description\"]).cumcount(ascending=False)\n",
    "        # Calculate the Current Amount Paid\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount_Total'] - combined_data['Previous_Payment_Amount_Total']\n",
    "        # Fill NaN for the first record in each group (no previous record exists)\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount'].fillna(combined_data['Payment_Amount_Total'])\n",
    "        # Revert the formatting of pull_date_initial column\n",
    "        combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "        # Calculate Payment Amount Percent and Payment Total Percent\n",
    "        combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "        combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "        table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "        correct_order = table_info['column_name'].tolist()\n",
    "        # Reorder the DataFrame to avoid conversion errors\n",
    "        combined_data = combined_data[correct_order]\n",
    "\n",
    "    else:\n",
    "        print(\"I am here\")\n",
    "        combined_data = scraped_data\n",
    "        # Post processing before loading into duckdb\n",
    "        combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "        combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "        # Group by Contract_Number, Contractor_Name and Project_Description (Since Ny has different jobs within the same contract number) and calculate the previous Payment_Amount_Total\n",
    "        combined_data['Previous_Payment_Amount_Total'] = combined_data.groupby([\"Contract_Number\",\"Contractor_Name\",\"Project_Description\"])[\"Payment_Amount_Total\"].shift(-1)\n",
    "        # Assign a reversed payment estimate number\n",
    "        combined_data[\"Payment_Number\"] = combined_data.groupby([\"Contract_Number\",\"Contractor_Name\",\"Project_Description\"]).cumcount(ascending=False)\n",
    "        # Calculate the Current Amount Paid\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount_Total'] - combined_data['Previous_Payment_Amount_Total']\n",
    "        # Fill NaN for the first record in each group (no previous record exists)\n",
    "        combined_data['Payment_Amount'] = combined_data['Payment_Amount'].fillna(combined_data['Payment_Amount_Total'])\n",
    "        # Revert the formatting of pull_date_initial column\n",
    "        combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "        # Calculate Payment Amount Percent and Payment Total Percent\n",
    "        combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "        combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "        table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "        correct_order = table_info['column_name'].tolist()\n",
    "        # Reorder the DataFrame to avoid conersion errors\n",
    "        combined_data = combined_data[correct_order]\n",
    "\n",
    "    # Replace the table with the updated data\n",
    "    con.execute(f\"DELETE FROM {table_name}\")\n",
    "    con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "    # Close connection\n",
    "    con.close()\n",
    "    send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "    print(\"NY City scraping completed and DUCKDB file updated Successfully.\")\n",
    "    logging.info(\n",
    "        'NY City scraping completed and DUCKDB file updated Successfully.')\n",
    "    ny_amendment.rename(columns = {'TRANSACTION TYPE': 'Transaction_Type','VENDOR NAME':'Contractor_Name','DEPARTMENT/FACILITY': 'Project_Dept_Facility','CONTRACT NUMBER':'Contract_Number','TRANSACTION AMOUNT':'Transaction_Amount',\n",
    "                        'START DATE':'Start_Date','END DATE':'Project_Comp_Substantial','DESCRIPTION':'Description',\n",
    "                        'TRANSACTION APPROVED/FILED DATE':'Transaction_Approved/Filed_Date'}, inplace=True)\n",
    "    ny_amendment.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\Monthly\\ny_amendment.xlsx\")\n",
    "ny_city_scraping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cbb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"NY_City\", \"data_store_NY\", \"NY_DOT\")\n",
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\NY_City\\Monthly\\ny_bulk_pipeline_april.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18a1c9",
   "metadata": {},
   "source": [
    "Georgia Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f03cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# URL of the site\n",
    "url = 'https://www.dot.ga.gov/GDOT/pages/contractors.aspx'\n",
    "\n",
    "# Set Chrome options to disable alerts\n",
    "chrome_options = Options() \n",
    "chrome_options.add_argument(\"--disable-popup-blocking\") \n",
    "chrome_options.add_argument(\"--disable-notifications\") \n",
    "chrome_options.add_argument(\"--disable-infobars\")\n",
    "chrome_options.add_argument(\"--disable-web-security\") \n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "\n",
    "# Contract number to search for\n",
    "# contract_numbers = list_ga[0:1000] # try with smaller list and clean '#' \n",
    "contract_numbers = ['B10007-97-M00-1','APER40-09-127-0'] #contract_id\n",
    "# contract_numbers = ['0000184.E3000', '0000301', '0000304']  #project_number\n",
    "# contract_numbers = ['0000078','0000088','0000083']  #PI_number   \n",
    "primary_counties = ['ALL COUNTIES']  # when value = all counties it is equivalent to the bulk option by primary counties\n",
    "vendor_names = ['BAKER INFRASTRUCTURE GROUP, INC.']      \n",
    "header_data_ga = [\"contract_id\", \"description\", \"source\", \"pi_numbers\", \"last_voucher_processed\", \"project_numbers\", \"current_contract_amount\", \"voucher/est_num\", \"voucher/est_date\", \"payment_percent_complete\",\"date_approved\",\"net_payment\",\"date_received\",\"balance\",\n",
    "                  \"date_processed\",\"material_allowance\",\"date_sent_to_accounting\", \"total_payments\", \"estimate_link\"]\n",
    "row_data_list_ga = []\n",
    "# Create a WebDriver instance with Chrome options\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "driver.get(url)\n",
    "original_window = driver.current_window_handle\n",
    "pi_number = False\n",
    "contract_id = False\n",
    "project_number = False\n",
    "primary_county = True\n",
    "vendor_name = False\n",
    "vendor_name_bulk = False # this option may trigger dos attack and lock the account. do not use yet\n",
    "try: #unexpected alerts need to be handled\n",
    "    \n",
    "    # Wait for psi_report to be present\n",
    "    WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.XPATH, '//button[@class=\"nb-expando__toggle js-expando__toggle\"]'))\n",
    "    )\n",
    "    psi_report = driver.find_elements(By.XPATH, '//button[@class=\"nb-expando__toggle js-expando__toggle\"]/div[@class=\"nb-expando__toggle-text\"]')\n",
    "    for j, row in enumerate(psi_report):\n",
    "        if j == 4:\n",
    "            row.click()\n",
    "    WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.LINK_TEXT, \"PSI\"))\n",
    "    )\n",
    "    # Navigate to payment dashboard\n",
    "    psi_link = driver.find_element(By.LINK_TEXT, \"PSI\")\n",
    "    psi_link.click()\n",
    "    # change the window of the driver since new tab is opened\n",
    "    for window_handle in driver.window_handles:\n",
    "        if window_handle != original_window:\n",
    "            driver.switch_to.window(window_handle)\n",
    "            break\n",
    "    # Navigate to payment window dashboard\n",
    "    payment_info = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, '//*[@id=\"dashboard_page_2_tab\"]/tbody/tr/td/div'))\n",
    "    )\n",
    "    payment_info.click()\n",
    "\n",
    "    # Retrive list of all primary counties\n",
    "    primary_county_dropdown = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[2]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "        )\n",
    "    primary_county_dropdown.click()\n",
    "    primary_county_labels = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '/html/body/div[9]/div/div[2]/div/div/label'))\n",
    "        ) \n",
    "    primary_county_list = [name.get_attribute('textContent').strip() for name in primary_county_labels if name.get_attribute('textContent').strip()]\n",
    "    \n",
    "    del primary_county_list[0]\n",
    "    primary_county_list = primary_county_list\n",
    "    primary_county_dropdown.click() # disengage the dropdown\n",
    "    # list to store individual dataframes\n",
    "    dataframes_payment = []\n",
    "    time.sleep(5)\n",
    "    def remove_selection():\n",
    "        remove_all_button = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"idShuttleContainerBody\"]/tr[3]/td[2]/table/tbody/tr[5]/td'))\n",
    "            ) \n",
    "        remove_all_button.click()\n",
    "\n",
    "    def check_conditions():\n",
    "        if contract_id:\n",
    "            # Find the contract ID dropdown\n",
    "            # Note: shortcut XPATH doesn't seem to be working because of the HTML structure. So full path is taken\n",
    "            contract_id_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[4]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img\"))\n",
    "            )\n",
    "            contract_id_dropdown.click()\n",
    "        if pi_number:\n",
    "            pi_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[3]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            pi_num_dropdown.click()\n",
    "            deselect_default = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[2]/div[1]/div/label'))\n",
    "            ) \n",
    "            deselect_default.click() \n",
    "        if project_number:\n",
    "            project_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[5]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            project_num_dropdown.click()\n",
    "        if primary_county:\n",
    "            primary_county_dropdown = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[2]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "                )\n",
    "            primary_county_dropdown.click()\n",
    "        if vendor_name or vendor_name_bulk:\n",
    "            vendor_name_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[1]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            vendor_name_dropdown.click()\n",
    "    def payment_info_selected():\n",
    "        if vendor_name_bulk:\n",
    "            move_all_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/table/tbody[1]/tr/td/div[2]/div/table/tbody/tr[3]/td[2]/table/tbody/tr[2]/td/img'))\n",
    "            )\n",
    "            for i in range(10):\n",
    "                move_all_button.click()\n",
    "\n",
    "        ok_button_2 = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/table/tbody[1]/tr/td/div[3]/a[1]'))\n",
    "        )\n",
    "        ok_button_2.click()\n",
    "        # Deselect \"default project\" value of 'PI Number' if not already deselected (that is in the case where the search is not by pi_number or primary county or vendor_name)\n",
    "        # Note: Again in this case shortcut XPATH isn't working so full path is used\n",
    "        if not pi_number and not primary_county and not vendor_name and not vendor_name_bulk:\n",
    "            ### This logic needs to be edited to account for when contract ID is true so that \"deselect_default\" is not triggered more than once (recommended : create a separate def for this logic)    \n",
    "            pi_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[3]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            pi_num_dropdown.click()\n",
    "            deselect_default = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/div[2]/div[1]/div/label'))\n",
    "            ) \n",
    "            deselect_default.click()\n",
    "        # Apply the filters\n",
    "        time.sleep(10)\n",
    "        apply = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"gobtn\"]'))\n",
    "        )\n",
    "\n",
    "        apply.click()\n",
    "        time.sleep(10)\n",
    "        try:\n",
    "            #switch the driver to iframe ID\n",
    "            iframe_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"d:dashboard~p:9297np0cm5nna3pq~x:m7no0s394ckllasi\"]'))\n",
    "            )\n",
    "            driver.switch_to.frame(iframe_element)\n",
    "            # format the report to html for interaction\n",
    "            format = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/table/tbody/tr/td/div/div/div/div/div[2]/div[1]/div/div[2]/a[1]/img[2]'))\n",
    "            )\n",
    "            format.click()\n",
    "            html = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '//*[@id=\"_xdoFMenu1\"]/div/div/ul/li[1]/div/a/div[2]'))\n",
    "            )\n",
    "            html.click()\n",
    "            #switch the driver to the next iframe ID\n",
    "            time.sleep(15)\n",
    "            iframe_element_html = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '//*[@id=\"xdo:docframe0\"]'))\n",
    "            )\n",
    "            driver.switch_to.frame(iframe_element_html)\n",
    "\n",
    "            # iterate and extract relevent data for all \n",
    "            # driver.switch_to.default_content()\n",
    "            tables = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '/html/body/table[@class=\"c29\"]'))\n",
    "            )\n",
    "\n",
    "            for i, row in enumerate(tables):\n",
    "                current_data = []\n",
    "                cells = row.find_elements(By.XPATH,'./tbody/tr/td/p/span')\n",
    "                siblings = row.find_elements(By.XPATH, './following-sibling::*')\n",
    "                cost_tables = []\n",
    "                counter = 0\n",
    "                for value in cells: \n",
    "                    if counter % 2 != 0:\n",
    "                        current_data.append(value.text)\n",
    "                    counter +=1\n",
    "                if len(current_data) != 7:\n",
    "                    print(\"check data of type1 table, iteration\", i)\n",
    "                for sibling in siblings:\n",
    "                    if not project_number and \"c39\" in sibling.get_attribute(\"class\"):\n",
    "                        cost_tables.append(sibling)\n",
    "                    elif project_number and \"c41\" in sibling.get_attribute(\"class\"):\n",
    "                        cost_tables.append(sibling)\n",
    "                    elif \"c29\" in sibling.get_attribute(\"class\"):\n",
    "                        break\n",
    "                for j, cost_table in enumerate(cost_tables):\n",
    "                    current_cost = []\n",
    "                    if not project_number:\n",
    "                        data_table = cost_table.find_element(By.XPATH,'./tbody/tr/td/table[@class=\"c37\"]')\n",
    "                        data = data_table.find_elements(By.XPATH,'./tbody/tr/td')\n",
    "                    elif project_number:\n",
    "                        data_table = cost_table.find_element(By.XPATH,'./tbody/tr/td/table[@class=\"c39\"]')\n",
    "                        data = data_table.find_elements(By.XPATH,'./tbody/tr/td')\n",
    "                    counter = 0\n",
    "                    for cell in data:\n",
    "                        link_element = cell.find_element(By.TAG_NAME, \"a\") if len(cell.find_elements(By.TAG_NAME, \"a\")) > 0 else None\n",
    "                        if counter % 2 != 0:\n",
    "                            if link_element:\n",
    "                                # Get the hyperlink (href attribute)\n",
    "                                link = link_element.get_attribute(\"href\")\n",
    "                                current_cost.append(link)\n",
    "                            else:\n",
    "                                current_cost.append(cell.text)\n",
    "                        counter +=1\n",
    "                    if len(current_cost) != 12:\n",
    "                        print(\"check data of type2 table, with following iterations of type1 and type2\", i, j)\n",
    "                    current_cost_join = current_data + current_cost\n",
    "                    row_data_list_ga.extend([current_cost_join])\n",
    "            current_df = pd.DataFrame(data=row_data_list_ga, columns = header_data_ga)\n",
    "            dataframes_payment.append(current_df)\n",
    "            # switch the driver back to default html content\n",
    "            driver.switch_to.default_content()\n",
    "        except TimeoutException:\n",
    "            print(\"Data not found for current input of contract numbers.\")\n",
    "\n",
    "\n",
    "    # Enter all contract_id's/vendor names/primary counties\n",
    "    if not primary_county and not vendor_name and not vendor_name_bulk:\n",
    "        for i in range(0,len(contract_numbers),100):\n",
    "            check_conditions() # corresponding dropdown is engaged\n",
    "            if i == 0:\n",
    "                search_more = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[3]/span'))\n",
    "                    )\n",
    "            else:\n",
    "                search_more = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/div[3]/span')) # search button path changes slightly after default is deselected which happens automatically by the second iteration\n",
    "                    )\n",
    "            search_more.click()\n",
    "            remove_selection() # removes selected filters from the previous iteration\n",
    "            edit_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.XPATH, '//*[@id=\"idShuttleContainerBody\"]/tr[1]/th[5]/a/img'))\n",
    "                )\n",
    "            edit_button.click() # opens a text box for entering required filters\n",
    "            contract_id_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "            )\n",
    "            value = contract_numbers[i:i + 100]\n",
    "            value = \"\\n\".join(value) # Create new lines for every value\n",
    "            contract_id_input.send_keys(value)\n",
    "            # county_input.send_keys(Keys.RETURN) # useful only if sending one value at a time that is to create a new line\n",
    "            ok_button_1 = driver.find_element(By.XPATH, '/html/body/div[12]/div/table/tbody[1]/tr/td/div[3]/a[1]')\n",
    "            ok_button_1.click() # Button to confirm values for filters after entering in the text box\n",
    "            payment_info_selected() # Retrieves the data from the selected filters and stores it in a dataframe\n",
    "        GA_DOT_payment_data = pd.concat(dataframes, ignore_index=True)\n",
    "    elif primary_county:\n",
    "        for i in range(0,len(primary_counties),1): # iterate through the list of primary counties that was intially fetched in steps of 20 (adjust as suitable)\n",
    "            check_conditions() # corresponding dropdown is engaged\n",
    "            # Navigate to Text area box for entering all the contract ID's, primary counties or any other filters\n",
    "            if i == 0:\n",
    "                search_more = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[3]/span'))\n",
    "                    )\n",
    "            else:\n",
    "                search_more = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/div[3]/span')) # search button path changes slightly after default is deselected which happens automatically by the second iteration\n",
    "                    )\n",
    "            search_more.click()\n",
    "            remove_selection() # removes selected filters from the previous iteration\n",
    "            edit_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.XPATH, '//*[@id=\"idShuttleContainerBody\"]/tr[1]/th[5]/a/img'))\n",
    "                )\n",
    "            edit_button.click() # opens a text box for entering required filters\n",
    "            county_input = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "                )\n",
    "            value = primary_counties[i:i + 1]\n",
    "            value = \"\\n\".join(value) # Create new lines for every value\n",
    "            county_input.send_keys(value)\n",
    "            # county_input.send_keys(Keys.RETURN) # useful only if sending one value at a time that is to create a new line\n",
    "            ok_button_1 = driver.find_element(By.XPATH, '/html/body/div[12]/div/table/tbody[1]/tr/td/div[3]/a[1]')\n",
    "            ok_button_1.click() # Button to confirm values for filters after entering in the text box\n",
    "            payment_info_selected() # Retrieves the data from the selected filters and stores it in a dataframe\n",
    "        GA_DOT_payment_data = pd.concat(dataframes_payment, ignore_index=True)\n",
    "\n",
    "    elif vendor_name: # no need to develop this functionality now, unless required in future\n",
    "        vendor_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "        )\n",
    "        for value in vendor_names: \n",
    "            vendor_input.send_keys(value)\n",
    "            vendor_input.send_keys(Keys.RETURN)\n",
    "            payment_info_selected()\n",
    "\n",
    "    # Fetch milestone dates information\n",
    "    dataframes_milestone = []\n",
    "    milestone_dates = WebDriverWait(driver, 10).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, '//*[@id=\"dashboard_page_0_tab\"]/tbody/tr/td/div'))\n",
    "    )\n",
    "    milestone_dates.click()\n",
    "    def check_conditions1():\n",
    "        if contract_id:\n",
    "            # Find the contract ID dropdown\n",
    "            # Note: shortcut XPATH doesn't seem to be working because of the HTML structure. So full path is taken\n",
    "            contract_id_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, \"/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[4]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img\"))\n",
    "            )\n",
    "            contract_id_dropdown.click()\n",
    "        if pi_number:\n",
    "            pi_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[3]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            pi_num_dropdown.click()\n",
    "            deselect_default = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[2]/div[1]/div/label'))\n",
    "            ) \n",
    "            deselect_default.click() \n",
    "        if project_number:\n",
    "            project_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[5]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            project_num_dropdown.click()\n",
    "        if primary_county:\n",
    "            primary_county_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[2]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            primary_county_dropdown.click()\n",
    "        if vendor_name or vendor_name_bulk:\n",
    "            vendor_name_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.visibility_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[1]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            vendor_name_dropdown.click()\n",
    "    def milestone_data_selected():\n",
    "        if vendor_name_bulk:\n",
    "            move_all_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/table/tbody[1]/tr/td/div[2]/div/table/tbody/tr[3]/td[2]/table/tbody/tr[2]/td/img'))\n",
    "            )\n",
    "            for i in range(10):\n",
    "                move_all_button.click()\n",
    "\n",
    "        ok_button_2 = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/table/tbody[1]/tr/td/div[3]/a[1]'))\n",
    "        )\n",
    "        ok_button_2.click()\n",
    "        # Deselect \"default project\" value of 'PI Number' if not already deselected (that is in the case where the search is not by pi_number or primary county or vendor_name)\n",
    "        # Note: Again in this case shortcut XPATH isn't working so full path is used\n",
    "        if not pi_number and not primary_county and not vendor_name and not vendor_name_bulk:    \n",
    "            pi_num_dropdown = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[7]/table[1]/tbody/tr/td[2]/div/table[1]/tbody/tr/td[2]/div[1]/table/tbody/tr/td[1]/div/table/tbody/tr[2]/td/div/table/tbody/tr/td/div/table/tbody/tr/td/div/div/div/table/tbody/tr/td/div/form/div/table/tbody/tr[2]/td/table/tbody/tr/td[3]/table/tbody/tr/td[2]/table/tbody/tr/td/div/div[1]/img'))\n",
    "            )\n",
    "            pi_num_dropdown.click()\n",
    "            deselect_default = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/div[2]/div[1]/div/label'))\n",
    "            ) \n",
    "            deselect_default.click()\n",
    "        # Apply the filters\n",
    "        time.sleep(10)\n",
    "        apply = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"gobtn\"]'))\n",
    "        )\n",
    "\n",
    "        apply.click()\n",
    "        time.sleep(10)\n",
    "        #switch the driver to iframe ID\n",
    "        iframe_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"d:dashboard~p:16a6r3787gg7lutu~x:93gsu676msc1raim\"]'))\n",
    "        )\n",
    "        driver.switch_to.frame(iframe_element)\n",
    "        # format the report to html for interaction\n",
    "        format = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, '/html/body/table/tbody/tr/td/div/div/div/div/div[2]/div[1]/div/div[2]/a[1]/img[2]'))\n",
    "        )\n",
    "        format.click()\n",
    "        html = WebDriverWait(driver, 10).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, '//*[@id=\"_xdoFMenu1\"]/div/div/ul/li[1]/div/a/div[2]'))\n",
    "        )\n",
    "        html.click()\n",
    "        #switch the driver to the next iframe ID\n",
    "        time.sleep(10)\n",
    "        iframe_element_next = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"xdo:docframe0\"]'))\n",
    "        )\n",
    "        driver.switch_to.frame(iframe_element_next)\n",
    "\n",
    "        # iterate and extract relevent data for all \n",
    "        # driver.switch_to.default_content()\n",
    "        tables = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '/html/body/table[@class=\"c7\"]'))\n",
    "        )\n",
    "\n",
    "        records = []\n",
    "\n",
    "        # Iterate through the tables in groups of 3\n",
    "        for i in range(0, len(tables), 3):\n",
    "            group_tables = tables[i:i+3]\n",
    "            if len(group_tables) < 3:\n",
    "                continue  # Skip incomplete groups\n",
    "\n",
    "            # Extract the vendor name from the first table\n",
    "            vendor_name = None\n",
    "            try:\n",
    "                vendor_name = group_tables[0].find_element(By.CLASS_NAME, \"c12\").text.strip()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Initialize a dictionary to store the combined data for this vendor\n",
    "            record = {'Vendor Name': vendor_name}\n",
    "            # Extract data from the second and third tables\n",
    "            for data_table in group_tables[1:]:\n",
    "                rows = data_table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            \n",
    "                for row in rows:\n",
    "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                    for idx in range(0, len(cells), 2):\n",
    "                        try:\n",
    "                            header = cells[idx].text.strip()\n",
    "                            value = cells[idx + 1].text.strip() if idx + 1 < len(cells) else None\n",
    "                            record[header] = value\n",
    "                        except IndexError:\n",
    "                            continue  # Skip incomplete rows\n",
    "\n",
    "            # Add the record to the list\n",
    "            records.extend([record])\n",
    "        records_df = pd.DataFrame(records)\n",
    "        dataframes_milestone.append(records_df) # Append the current iteration dataframe to the list \"dataframes1\"\n",
    "        # switch the driver back to default html content\n",
    "        driver.switch_to.default_content()\n",
    "    \n",
    "    # Enter all contract_id's/vendor names/primary counties\n",
    "    if not primary_county and not vendor_name and not vendor_name_bulk:\n",
    "        for i in range(0,len(contract_numbers),100):\n",
    "            check_conditions1() # corresponding dropdown is engaged\n",
    "            # Navigate to Text area box for entering all the contract ID's\n",
    "            search_more = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[10]/div/div[3]/span'))\n",
    "                )\n",
    "            search_more.click()\n",
    "            remove_selection()\n",
    "            edit_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.XPATH, '//*[@id=\"idShuttleContainerBody\"]/tr[1]/th[5]/a/img'))\n",
    "                )\n",
    "            edit_button.click()\n",
    "            county_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "            )\n",
    "            value = primary_counties[i:i + 100]\n",
    "            value = \"\\n\".join(value) # Create new lines for every value\n",
    "            county_input.send_keys(value)\n",
    "            ok_button_1 = driver.find_element(By.XPATH, '/html/body/div[12]/div/table/tbody[1]/tr/td/div[3]/a[1]')\n",
    "            ok_button_1.click()\n",
    "            milestone_data_selected()\n",
    "        GA_DOT_milestone_data = pd.concat(dataframes_milestone, ignore_index=True)\n",
    "    elif primary_county:\n",
    "        for i in range(0,len(primary_counties),1): # iterate through the list of primary counties that was intially fetched in steps of 20\n",
    "            check_conditions1() # corresponding dropdown is engaged\n",
    "            # Navigate to Text area box for entering all the contract ID's\n",
    "            search_more = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[3]/span'))\n",
    "                )\n",
    "            search_more.click()\n",
    "            remove_selection()\n",
    "            edit_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.visibility_of_element_located((By.XPATH, '//*[@id=\"idShuttleContainerBody\"]/tr[1]/th[5]/a/img'))\n",
    "                )\n",
    "            edit_button.click()\n",
    "            county_input = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "            )\n",
    "            value = primary_counties[i:i + 1]\n",
    "            value = \"\\n\".join(value) # Create new lines for every value\n",
    "            county_input.send_keys(value)\n",
    "            ok_button_1 = driver.find_element(By.XPATH, '/html/body/div[12]/div/table/tbody[1]/tr/td/div[3]/a[1]')\n",
    "            ok_button_1.click()\n",
    "            milestone_data_selected()\n",
    "        GA_DOT_milestone_data = pd.concat(dataframes_milestone, ignore_index=True)\n",
    "    elif vendor_name: # no need to develop this functionality now, unless required in future\n",
    "        vendor_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, '//*[@id=\"shuttleSelectEditTextArea\"]'))\n",
    "        )\n",
    "        for value in vendor_names: \n",
    "            vendor_input.send_keys(value)\n",
    "            vendor_input.send_keys(Keys.RETURN)\n",
    "\n",
    " \n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b685ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join payment info data and milestone dates data\n",
    "GA_DOT_data = pd.merge(GA_DOT_payment_data,GA_DOT_milestone_data, left_on=\"contract_id\", right_on=\"ContractID:\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c352952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST PROCESSING\n",
    "df = GA_DOT_data.copy()\n",
    "df.rename(columns = {'contract_id':'Contract_Number','project_numbers':'Project_Number','description':'Project_Description','Let Date:':'Project_Letting','current_contract_amount':'Project_Cost_Total','Original Contract Completion Date:':'Project_Comp_Est',\n",
    "                     'Current Specified Completion Date:':'Project_Comp_Substantial','source': 'Contract_Source','voucher/est_num':'Payment_Number','net_payment':'Payment_Amount','voucher/est_date':'Payment_Date','total_payments':'Payment_Amount_Total',\n",
    "                     'payment_percent_complete':'Payment_Total_Percent','date_approved':'Payment_Approved','date_received':'Payment_Received','date_processed':'Payment_Processed','date_sent_to_accounting':'Payment_toAccounting',\n",
    "                     'material_allowance':'Material_Allowance','balance':'Payment_Balance','estimate_link':'Comments','Vendor Name':'Contractor_Name','Material Certificate:':'Material_Certificate',\n",
    "                     'Punch List to Contractor:':'Project_PunchList_Sent','Punch List Complete:':'Project_PunchList_Complete','Closing Conference:':'Project_Closing_Conference','Time Charges Stopped:':'Project_Charges_Stopped',\n",
    "                     'Contractor Sent Final Quantities:':'Project_FinalQuantities_Sent','Contractor Accepts Final Quantities:':'Project_FinalQuantities_Accept','Date Accepted':'Project_Date_Accepted'}, inplace=True)\n",
    "\n",
    "# Drop duplicate and irrelevant columns\n",
    "df.drop(['pi_numbers','last_voucher_processed','ContractID:','Description:','Project#(s):','PI Numbers(s):','Source:'], axis=1, inplace=True)\n",
    "\n",
    "def parse_money(value): \n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value)\n",
    "df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "df['Payment_Balance'] = df['Payment_Balance'].apply(parse_money)\n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "df['Material_Allowance'] = df['Material_Allowance'].apply(parse_money)\n",
    "\n",
    "# Convert all date columns to a standard format MM-DD-YYYY\n",
    "df['Project_Comp_Est'] = pd.to_datetime(df['Project_Comp_Est'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_Comp_Substantial'] = pd.to_datetime(df['Project_Comp_Substantial'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Payment_Date'] = pd.to_datetime(df['Payment_Date'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Payment_Approved'] = pd.to_datetime(df['Payment_Approved'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Payment_Received'] = pd.to_datetime(df['Payment_Received'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Payment_Processed'] = pd.to_datetime(df['Payment_Processed'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Payment_toAccounting'] = pd.to_datetime(df['Payment_toAccounting'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_Letting'] = pd.to_datetime(df['Project_Letting'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_PunchList_Sent'] = pd.to_datetime(df['Project_PunchList_Sent'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_PunchList_Complete'] = pd.to_datetime(df['Project_PunchList_Complete'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_Closing_Conference'] = pd.to_datetime(df['Project_Closing_Conference'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_Charges_Stopped'] = pd.to_datetime(df['Project_Charges_Stopped'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_FinalQuantities_Sent'] = pd.to_datetime(df['Project_FinalQuantities_Sent'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_FinalQuantities_Accept'] = pd.to_datetime(df['Project_FinalQuantities_Accept'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Project_Date_Accepted'] = pd.to_datetime(df['Project_Date_Accepted'],format='%b-%d-%Y',errors='coerce')\n",
    "df['Material_Certificate'] = pd.to_datetime(df['Material_Certificate'],format='%b-%d-%Y',errors='coerce')\n",
    "\n",
    "\n",
    "df['Project_Comp_Est'] = df['Project_Comp_Est'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_Comp_Substantial'] = df['Project_Comp_Substantial'].dt.strftime('%m-%d-%Y')\n",
    "df['Payment_Date'] = df['Payment_Date'].dt.strftime('%m-%d-%Y')\n",
    "df['Payment_Approved'] = df['Payment_Approved'].dt.strftime('%m-%d-%Y')\n",
    "df['Payment_Received'] = df['Payment_Received'].dt.strftime('%m-%d-%Y')\n",
    "df['Payment_Processed'] = df['Payment_Processed'].dt.strftime('%m-%d-%Y')\n",
    "df['Payment_toAccounting'] = df['Payment_toAccounting'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_Letting'] = df['Project_Letting'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_PunchList_Sent'] = df['Project_PunchList_Sent'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_PunchList_Complete'] = df['Project_PunchList_Complete'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_Closing_Conference'] = df['Project_Closing_Conference'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_Charges_Stopped'] = df['Project_Charges_Stopped'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_FinalQuantities_Sent'] = df['Project_FinalQuantities_Sent'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_FinalQuantities_Accept'] = df['Project_FinalQuantities_Accept'].dt.strftime('%m-%d-%Y')\n",
    "df['Project_Date_Accepted'] = df['Project_Date_Accepted'].dt.strftime('%m-%d-%Y')\n",
    "df['Material_Certificate'] = df['Material_Certificate'].dt.strftime('%m-%d-%Y')\n",
    "\n",
    "# convert \"Payment_Number\" column to integer type to avoid duplicate records when integrated with duckdb\n",
    "df['Payment_Number'] = df['Payment_Number'].astype(int)\n",
    "\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Georgia\\data_store_GA.duckdb\"\n",
    "table_name = \"GA_DOT\"\n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Contractor_Name  TEXT,\n",
    "Contract_Number  TEXT,\n",
    "Project_Number  TEXT,\n",
    "Contract_Source\t TEXT,\n",
    "Payment_Number  INTEGER,\n",
    "Project_Description\tTEXT,\n",
    "Project_Comp_Est  TEXT,\n",
    "Project_Comp_Substantial  TEXT,\t\n",
    "\n",
    "Payment_Amount DOUBLE,\t\n",
    "Project_Cost_Total  DOUBLE,\t\n",
    "Payment_Amount_Total  DOUBLE,\n",
    "Payment_Total_Percent  TEXT,\n",
    "Payment_Amount_Percent  FLOAT,\n",
    "Payment_Balance  DOUBLE,\n",
    "Material_Allowance DOUBLE,\n",
    "\n",
    "Pull_Date_Initial TEXT,\n",
    "Payment_Date  TEXT,\n",
    "Payment_Approved  TEXT,\n",
    "Payment_Received  TEXT,\n",
    "Payment_Processed  TEXT,\n",
    "Payment_toAccounting  TEXT,\n",
    "Comments  TEXT,\n",
    "\n",
    "Project_Letting TEXT,\n",
    "Material_Certificate  TEXT,\t\n",
    "Project_PunchList_Sent  TEXT,\n",
    "Project_PunchList_Complete\tTEXT,\n",
    "Project_Closing_Conference  TEXT,\n",
    "Project_Charges_Stopped  TEXT,\n",
    "Project_FinalQuantities_Sent  TEXT,\n",
    "Project_FinalQuantities_Accept TEXT,\t\n",
    "Project_Date_Accepted TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial','Payment_Amount_Percent'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conersion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "else:\n",
    "    combined_data = scraped_data\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conersion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "# send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"Georgia scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'Georgia scraping completed and DUCKDB file updated Successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"Georgia\", \"data_store_ga\", \"GA_DOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Georgia\\Monthly\\ga_bulk_pipeline_aprilS.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff815cc9",
   "metadata": {},
   "source": [
    "Oklahoma Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7be8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "# URL of the site\n",
    "url = 'https://www.odot.org/CONTRACTADMIN/ESTIMATES/'\n",
    "\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "SMTP_PORT = 587  # Usually 587 for TLS\n",
    "EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "def send_email(subject, body):\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = EMAIL_SENDER\n",
    "        msg['To'] = EMAIL_RECIPIENT\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        logging.info(\"Email notification sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email notification: {e}\")\n",
    "        logging.error(f\"Failed to send email notification: {e}\")\n",
    "\n",
    "# contract_numbers_list = test_list_ok[773:1070] \n",
    "bulk_contracts = True\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "original_window = driver.window_handles[0]\n",
    "row_data_list_ok = []\n",
    "header_data_ok = [\"contract_id\", \"payment_number\",\"date_let\",\"ntp_eff_date\",\"pay_period\",\"date_awarded\",\"date_work_began\",\"org_cont_time\",\"date_cont_executed\",\"date_time_stopped\",\"current_time_charged\",\"date_ntp_issued\",\n",
    "                  \"completion_date\",\"current_time_allowed\",\"general_liability_exp\",\"workman_comp_exp\",\"percent_time_used\",\"spec_year\",\"date_approved\",\"bid_amount\",\"funds_available_bid_co\",\"percent_complete\",\n",
    "                  \"unearned_balance\", \"total_to_date\",\"prev_to_date\",\"this_estimate\", \"project_numbers\", \"primary_job_num\", \"cont_desp\",\"primary_county\",\"road_name\",\"prime_cont\",\"surety_company\"]\n",
    "\n",
    "try:\n",
    "    if bulk_contracts:\n",
    "        all_contracts = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"main\"]/div/a'))\n",
    "        )\n",
    "        all_contracts_list = []\n",
    "        for row in all_contracts:\n",
    "            all_contracts_list.append(row.text)\n",
    "        contract_numbers = all_contracts_list\n",
    "    elif not bulk_contracts:\n",
    "        # contract_numbers = contract_numbers_list\n",
    "        contract_numbers = contract_numbers\n",
    "\n",
    "    for i,value in enumerate(contract_numbers[614:624]):\n",
    "        \n",
    "        # Toggle the radio button filter\n",
    "        contract_id_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"filt1\"))\n",
    "        )\n",
    "        contract_id_button.click()\n",
    "\n",
    "        # Find the contract number input box and enter the contract number\n",
    "        contract_id_input = driver.find_element(By.ID, \"filt1v\")\n",
    "        contract_id_input.clear()\n",
    "        contract_id_input.send_keys(value)\n",
    "        apply = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"menuBar\"]/input[8]'))\n",
    "        )\n",
    "        apply.click()\n",
    "        # Submit the search form\n",
    "        try:\n",
    "            contract_id_link = WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, value))\n",
    "            )\n",
    "            contract_id_link.click()            \n",
    "            all_reports = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"main\"]/table/tbody/tr/td'))\n",
    "            )\n",
    "            report_counter = 0\n",
    "            for i in range(len(all_reports) - 1, -1, -1): #iterate backwards through the list\n",
    "                if report_counter > 3: # fetch last 4 reports only since historical data has been scraped already\n",
    "                    break\n",
    "                row = all_reports[i].find_element(By.XPATH,\"./a\")\n",
    "                if \"index\" not in row.get_attribute('textContent') and row.get_attribute('textContent').isdigit(): # if only the latest report is needed, the loop should be ended the first time the condition is satisfied. conditional logic can be improved\n",
    "                    est_num = row.get_attribute('textContent')\n",
    "                    report = row\n",
    "                    report.click()\n",
    "                    time.sleep(1) # addresses race conditions. Expected issue to be resolved- WebDriverException: no such execution context \n",
    "                    # change the window of the driver since new tab is opened\n",
    "                    new_window_handle = [handle for handle in driver.window_handles if handle != original_window][-1]\n",
    "                    driver.switch_to.window(new_window_handle)\n",
    "                    table = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_all_elements_located((By.XPATH, '/html/body/table[3]/tbody/tr'))\n",
    "                    )\n",
    "            \n",
    "                    current_data = []\n",
    "                    current_data.append(value)\n",
    "                    current_data.append(est_num)\n",
    "                    for row in table:\n",
    "                        cells = row.find_elements(By.XPATH, './td')\n",
    "                        counter = 0\n",
    "                        for cell in cells:\n",
    "                            if counter % 2 != 0: # If the iteration is odd\n",
    "                                current_data.append(cell.text)\n",
    "                            counter += 1\n",
    "                    if len(current_data) < 18:\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(original_window)\n",
    "                        continue\n",
    "                    del current_data[18] # Removes extra empty values from empty cells\n",
    "                    \n",
    "                    bid_amount = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[1]/td[2]').text\n",
    "                    funds_available = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[2]/td[2]').text\n",
    "                    percent_complete = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[3]/td[2]').text\n",
    "                    unearned_balance = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[4]/td[2]').text\n",
    "                    total_to_date = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[9]/td[4]').text\n",
    "                    prev_to_date = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[9]/td[5]').text\n",
    "                    this_estimate = driver.find_element(By.XPATH, '/html/body/table[4]/tbody/tr[9]/td[6]').text\n",
    "\n",
    "                    current_data.append(bid_amount)\n",
    "                    current_data.append(funds_available)\n",
    "                    current_data.append(percent_complete)\n",
    "                    current_data.append(unearned_balance)\n",
    "                    current_data.append(total_to_date)\n",
    "                    current_data.append(prev_to_date)\n",
    "                    current_data.append(this_estimate)\n",
    "                    \n",
    "\n",
    "                    additional_info_table = WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_all_elements_located((By.XPATH, '/html/body/table[2]/tbody/tr/td[2]'))\n",
    "                    )\n",
    "                    for j, row in enumerate(additional_info_table):\n",
    "                        current_data.append(row.text)\n",
    "                    del current_data[-3:-1]\n",
    "                    if len(current_data) != 33:\n",
    "                        print(\"check data for contract id\", value)\n",
    "                    else:\n",
    "                        row_data_list_ok.extend([current_data])\n",
    "                    driver.close()\n",
    "                    driver.switch_to.window(original_window)\n",
    "                    report_counter += 1\n",
    "            est_portal_link = WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.LINK_TEXT, 'Estimates Portal'))\n",
    "                )\n",
    "            est_portal_link.click()\n",
    "            \n",
    "        except TimeoutException:    \n",
    "            print(value,\"Not Found\")\n",
    "        except InvalidArgumentException:   # When true it indicates that the input contract ID is a numeric data type and needs to be changed to string\n",
    "            print(value,\"Not Found - change to string dtype\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "# Create a dataframe   \n",
    "OK_DOT_Data = pd.DataFrame(data=row_data_list_ok, columns = header_data_ok)\n",
    "# POST PROCESSING\n",
    "df = OK_DOT_Data.copy()\n",
    "df.rename(columns = {'contract_id':'Contract_Number','payment_number':'Payment_Number','project_numbers':'Project_Number','date_let':'Project_Letting',\n",
    "                     'date_awarded':'Project_Bid_Awarded','ntp_eff_date':'Project_Start_Effective','date_work_began':'Project_Start_Actual',\n",
    "                     'cont_desp':'Project_Description','pay_period':'Payment_Work_Period','bid_amount':'Project_Cost_Total','funds_available_bid_co':'Project_Cost_Adjusted',\n",
    "                     'completion_date':'Project_Comp_Final','date_time_stopped':'Project_Charges_Stopped','surety_company':'Bond_Company_Name',\n",
    "                     'prime_cont':'Contractor_Name','this_estimate':'Payment_Amount','date_approved':'Payment_Approved','total_to_date':'Payment_Amount_Total',\n",
    "                     'unearned_balance':'Payment_Balance','percent_complete':'Payment_Total_Percent'}, inplace=True)\n",
    "\n",
    "\n",
    "def parse_money(value):\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value)\n",
    "\n",
    "df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "df['Project_Cost_Adjusted'] = df['Project_Cost_Adjusted'].apply(parse_money)\n",
    "df['Payment_Balance'] = df['Payment_Balance'].apply(parse_money)\n",
    "df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "df['prev_to_date'] = df['prev_to_date'].apply(parse_money)\n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "\n",
    "# convert \"Payment_Number\" column to integer type to avoid duplicate records when integrated with duckdb\n",
    "df['Payment_Number'] = df['Payment_Number'].astype(int)\n",
    "\n",
    "df['org_cont_time'] = pd.to_numeric(df['org_cont_time'], errors='coerce')\n",
    "df['current_time_allowed'] = pd.to_numeric(df['current_time_allowed'], errors='coerce')\n",
    "\n",
    "# replace 0 with blanks for the following date columns\n",
    "df[['general_liability_exp', 'workman_comp_exp']] = df[['general_liability_exp', 'workman_comp_exp']].replace(0, '')\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Oklahoma\\data_store_OK.duckdb\"\n",
    "table_name = \"OK_DOT\"\n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df.copy()\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "# Fields to be calculated\n",
    "\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Contract_Number  TEXT,\n",
    "Project_Number  TEXT,\n",
    "primary_job_num TEXT,\n",
    "Payment_Number  INTEGER,\n",
    "Project_Description\tTEXT,\n",
    "primary_county TEXT,\n",
    "Contractor_Name TEXT,\n",
    "Bond_Company_Name TEXT,\n",
    "road_name TEXT,\n",
    "\n",
    "org_cont_time DOUBLE,\n",
    "date_cont_executed TEXT,\n",
    "Project_Charges_Stopped TEXT,\n",
    "current_time_charged TEXT,\n",
    "current_time_allowed DOUBLE,\n",
    "general_liability_exp TEXT,\n",
    "workman_comp_exp TEXT,\n",
    "percent_time_used TEXT,\n",
    "spec_year TEXT,\n",
    "\n",
    "Project_Cost_Total DOUBLE,\n",
    "Project_Cost_Adjusted DOUBLE,\n",
    "Project_Cost_Additions DOUBLE,\n",
    "\n",
    "Payment_Balance DOUBLE,\n",
    "Payment_Amount_Total DOUBLE,\n",
    "prev_to_date DOUBLE,\n",
    "Payment_Amount DOUBLE,\n",
    "Payment_Total_Percent TEXT,\n",
    "Payment_Amount_Percent DOUBLE,\n",
    "\n",
    "\n",
    "Pull_Date_Initial TEXT,\n",
    "Payment_Approved TEXT,\n",
    "Payment_Date TEXT,\n",
    "Project_Letting TEXT,\n",
    "Project_Bid_Awarded TEXT,\n",
    "Project_Start_Effective TEXT,\n",
    "Project_Start_Actual TEXT,\n",
    "Payment_Work_Period TEXT,\n",
    "Project_Comp_Final TEXT,\n",
    "date_ntp_issued TEXT,\n",
    "Project_Comp_Est TEXT,\n",
    "Project_Comp_Substantial TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial','Project_Cost_Additions','Payment_Amount_Percent'\n",
    "                                                                                     'Project_Comp_Est','Project_Comp_Substantial','Payment_Date'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Number\"], ascending=[True,False])\n",
    "\n",
    "    # Calculate derived fields safely\n",
    "    combined_data['Project_Cost_Additions'] = combined_data['Project_Cost_Adjusted'] - combined_data['Project_Cost_Total']\n",
    "    combined_data['Payment_Amount_Percent'] = round(\n",
    "    combined_data['Payment_Amount'] / combined_data['Payment_Amount_Total'].replace(0, pd.NA) * 100, 2\n",
    "    )\n",
    "    # Extract the last date string using string slicing (equivalent to SUBSTR)\n",
    "    combined_data['Payment_Date'] = combined_data['Payment_Work_Period'].str[-10:]\n",
    "\n",
    "    # Convert and calculate date-based fields\n",
    "    combined_data['date_ntp_issued'] = pd.to_datetime(combined_data['date_ntp_issued'], errors='coerce')\n",
    "    combined_data['Project_Comp_Est'] = combined_data['date_ntp_issued'] + pd.to_timedelta(combined_data['org_cont_time'], unit='D')\n",
    "    combined_data['Project_Comp_Substantial'] = combined_data['date_ntp_issued'] + pd.to_timedelta(combined_data['current_time_allowed'], unit='D')\n",
    "\n",
    "    # Format dates back to strings\n",
    "    combined_data['Project_Comp_Est'] = combined_data['Project_Comp_Est'].dt.strftime('%m/%d/%Y')\n",
    "    combined_data['Project_Comp_Substantial'] = combined_data['Project_Comp_Substantial'].dt.strftime('%m/%d/%Y')\n",
    "    combined_data['date_ntp_issued'] = combined_data['date_ntp_issued'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "else:\n",
    "    combined_data = scraped_data.copy()\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Number\"], ascending=[True,False])\n",
    "    # Calculate derived fields safely\n",
    "    combined_data['Project_Cost_Additions'] = combined_data['Project_Cost_Adjusted'] - combined_data['Project_Cost_Total']\n",
    "    combined_data['Payment_Amount_Percent'] = round(\n",
    "    combined_data['Payment_Amount'] / combined_data['Payment_Amount_Total'].replace(0, pd.NA) * 100, 2\n",
    "    )\n",
    "    # Extract the last date string using string slicing (equivalent to SUBSTR)\n",
    "    combined_data['Payment_Date'] = combined_data['Payment_Work_Period'].str[-10:]\n",
    "\n",
    "    # Convert and calculate date-based fields\n",
    "    combined_data['date_ntp_issued'] = pd.to_datetime(combined_data['date_ntp_issued'], errors='coerce')\n",
    "    combined_data['Project_Comp_Est'] = combined_data['date_ntp_issued'] + pd.to_timedelta(combined_data['org_cont_time'], unit='D')\n",
    "    combined_data['Project_Comp_Substantial'] = combined_data['date_ntp_issued'] + pd.to_timedelta(combined_data['current_time_allowed'], unit='D')\n",
    "\n",
    "    # Format dates back to strings\n",
    "    combined_data['Project_Comp_Est'] = combined_data['Project_Comp_Est'].dt.strftime('%m/%d/%Y')\n",
    "    combined_data['Project_Comp_Substantial'] = combined_data['Project_Comp_Substantial'].dt.strftime('%m/%d/%Y')\n",
    "    combined_data['date_ntp_issued'] = combined_data['date_ntp_issued'].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data.shape)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"Oklahoma scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'Oklahoma scraping completed and DUCKDB file updated Successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"Oklahoma\", \"data_store_OK\", \"OK_DOT\")                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Oklahoma\\Monthly\\OK_bulk_pipeline_april.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8363185",
   "metadata": {},
   "source": [
    "Washington Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "SMTP_PORT = 587  # Usually 587 for TLS\n",
    "EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "def send_email(subject, body):\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = EMAIL_SENDER\n",
    "        msg['To'] = EMAIL_RECIPIENT\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        logging.info(\"Email notification sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email notification: {e}\")\n",
    "        logging.error(f\"Failed to send email notification: {e}\")\n",
    "# Type the desired file name and save\n",
    "file_path = r\"C:\\Users\\TarunPongulaty\\Downloads\\wa_active_contracts_list.pdf\" \n",
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "# Set up the WebDriver\n",
    "options = webdriver.ChromeOptions() \n",
    "options.add_argument(\"--start-maximized\") # Ensures the browser is fullscreen\n",
    "# URL of the site\n",
    "url = 'https://www.wsdot.wa.gov/publications/fulltext/construction/projectreports/Active.pdf-en-us.pdf'\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(url)\n",
    "# original_window = driver.window_handles[0]\n",
    "try:\n",
    "        time.sleep(3)\n",
    "        # Simulate pressing Ctrl+S to open \"Save As\" dialog\n",
    "        pyautogui.hotkey('ctrl', 's')\n",
    "        time.sleep(2)\n",
    "         # Update with your desired location and name\n",
    "        pyautogui.typewrite(file_path)\n",
    "        time.sleep(1)  # Small delay for typing\n",
    "\n",
    "        # Press 'Enter' to save\n",
    "        pyautogui.press('enter')\n",
    "\n",
    "        time.sleep(2)\n",
    "        pyautogui.press('left')  # Move focus to the \"Yes\" button\n",
    "        time.sleep(0.5)  # Small delay for smooth navigation\n",
    "        pyautogui.press('enter')\n",
    "\n",
    "        # Wait for the file to be saved\n",
    "        time.sleep(5)\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "#working pdf reader\n",
    "\n",
    "tables = tabula.read_pdf(file_path, pages='all', multiple_tables=True)\n",
    "df = pd.concat(tables, ignore_index=True)\n",
    "df['Contract\\rNumber'] = df['Contract\\rNumber'].astype(str).apply(lambda x: x.split('.')[0].zfill(6)) # makes sure leading zeros are retained by fixing the length of ID to 6. In the website it is mentioned that the contract numbers are 6 in length\n",
    "contract_numbers_wa = df['Contract\\rNumber'].to_list() # 'split('.')[0]' is used above as the pdf reader adds decimals to numbers in some cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# URL of the site\n",
    "url = 'https://remoteapps.wsdot.wa.gov/construction/project/progress/'\n",
    "\n",
    "# Contract number to search for\n",
    "contract_numbers = contract_numbers_wa \n",
    "# contract_numbers = ['009446']           \n",
    "header_data_wa = []\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "row_data_list_wa = []\n",
    "try:\n",
    "    for i,value in enumerate(contract_numbers):\n",
    "        # Wait for the contract number input box to be present\n",
    "    \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"txtContractNumber\"))\n",
    "        )\n",
    "\n",
    "        # Find the contract number input box and enter the contract number\n",
    "        contract_number_input = driver.find_element(By.ID, \"txtContractNumber\")\n",
    "        contract_number_input.clear()\n",
    "        contract_number_input.send_keys(value)\n",
    "        \n",
    "\n",
    "        # Submit the search form\n",
    "        contract_number_input.send_keys(Keys.RETURN)\n",
    "        # Locate the dropdown element for payment dates\n",
    "        try:\n",
    "            payment_date_options = WebDriverWait(driver, 100).until(\n",
    "                        EC.visibility_of_all_elements_located((By.XPATH, '//*[@id=\"select-payment-date\"]/option'))\n",
    "                    )\n",
    "            report_counter = 0\n",
    "            # iterate over the payment date options:\n",
    "            for option in payment_date_options:\n",
    "                if report_counter > 4: # fetch last 4 reports only since historical data has been scraped already\n",
    "                    break\n",
    "                option.click()\n",
    "                try:\n",
    "                    WebDriverWait(driver, 300).until(\n",
    "                        EC.presence_of_element_located((By.ID, 'Page1'))\n",
    "                    )\n",
    "                    table1 = WebDriverWait(driver, 300).until(\n",
    "                        EC.visibility_of_element_located((By.XPATH, '//table[@class=\"S45\"]'))\n",
    "                    )\n",
    "                    # Find all headers from 2 tables and append them\n",
    "                    if not header_data_wa:\n",
    "                        # Find all cells in the row\n",
    "                        for row in table1.find_elements(By.XPATH,'.//tr'):    \n",
    "                            cells = row.find_elements(By.XPATH,'.//td')    \n",
    "                            # Extract headers only from table 1    \n",
    "                            header_data_wa.extend([cells[0].get_attribute(\"textContent\").strip()]) \n",
    "                        del header_data_wa[2]               \n",
    "                        # Extract header only from table 2    \n",
    "                        header = driver.find_element(By.XPATH,'//table[@id=\"List1\"]/tbody/tr[last()]/td')\n",
    "                        header_data_wa.extend([header.get_attribute(\"textContent\").strip()])\n",
    "\n",
    "                    # Extract row data from table 1 and 2\n",
    "                    row_data_list = []\n",
    "                    for row in table1.find_elements(By.XPATH,'.//tr'):    \n",
    "                        cells = row.find_elements(By.XPATH,'.//td')   \n",
    "                        # Extract records only from table 1 \n",
    "                        if len(cells) >=2:          \n",
    "                            values = cells[1].get_attribute(\"textContent\")\n",
    "                            if not values:\n",
    "                                values = \"N/A\"  # Use a default value\n",
    "                        if any(values):\n",
    "                            row_data_list.append(values)\n",
    "                        \n",
    "                    del row_data_list[2] # Deletes the empty element extracted  due to extra tag containing just space in html structure\n",
    "                    # Extract total from table 2\n",
    "                    row_data = driver.find_element(By.XPATH,'//table[@id=\"List1\"]/tbody/tr[last()]/td[last()]')\n",
    "                    row_data_list.append(row_data.get_attribute(\"textContent\"))\n",
    "                    row_data_list_wa.extend([row_data_list])\n",
    "                    report_counter += 1\n",
    "        \n",
    "                except NoSuchElementException:\n",
    "                    print(\"No Data for\", value)    \n",
    "                except TimeoutException:\n",
    "                    print(value,\"some payment estimate data Not Found\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing payment estimate: {e}\")\n",
    "        except TimeoutException:\n",
    "                    print(\"Not Found for\", value)\n",
    "        \n",
    "        # delay_seconds = random.uniform(2,10)\n",
    "        # time.sleep(delay_seconds)\n",
    "        driver.get(url)\n",
    "except TimeoutException:\n",
    "    print(\"Website not responding. Re-run\")\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "WA_DOT_data = pd.DataFrame(data=row_data_list_wa, columns = header_data_wa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc2549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST PROCESSING\n",
    "df = WA_DOT_data.copy()\n",
    "\n",
    "# Split \"Contract Number:\" into 'Contract_Number' and 'Project_Description'\n",
    "split_columns = df['Contract Number:'].str.split(' ', n=1, expand=True)\n",
    "split_columns.columns = ['Contract_Number', 'Project_Description']\n",
    "df = pd.concat([df, split_columns], axis = 1)\n",
    "del df['Contract Number:']\n",
    "\n",
    "\n",
    "df.rename(columns = {'Prime Contractor:':'Contractor_Name','Project Engineer:':'Engineer_Name','Estimate Number(s):': 'Estimate_Number_Original',\n",
    "                     'Payment Date:':'Payment_Date','Overall - Total':'Payment_Amount','Last Date of Work This Payment:':'Last_Date_of_Work_This_Payment'}, inplace=True)\n",
    "\n",
    "# Create payment work period column from prior last date of work this payment to payment date\n",
    "df[\"Payment_Work_Period\"] = df[\"Last_Date_of_Work_This_Payment\"].str.cat(df[\"Payment_Date\"], sep=\" to \", na_rep=\"Unknown\")\n",
    "\n",
    "def parse_money(value):\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value)\n",
    "\n",
    "# format money columns to numeric for calculations ahead\n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "\n",
    "# Create scraping pull date column\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Washington\\data_store_WA.duckdb\"\n",
    "table_name = \"WA_DOT\"\n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Contract_Number  TEXT,\n",
    "Contractor_Name  TEXT,\n",
    "Engineer_Name  TEXT,\n",
    "Payment_Number  INT,\n",
    "Estimate_Number_Original  TEXT,\n",
    "Project_Description\tTEXT,\n",
    "Payment_Date  TEXT,\n",
    "Last_Date_of_Work_This_Payment  TEXT,\n",
    "Payment_Work_Period  TEXT,\n",
    "Payment_Amount DOUBLE,\t\n",
    "Payment_Amount_Total  DOUBLE,\n",
    "Project_Cost_Total\tDOUBLE,\n",
    "Payment_Balance DOUBLE,\n",
    "Pull_Date_Initial TEXT,\n",
    "Payment_Amount_Percent FLOAT,\n",
    "Payment_Total_Percent  FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    combined_data['Estimate_Number_Original'] = combined_data['Estimate_Number_Original'].str.strip()\n",
    "    # find duplicates by all columns except the columns below. Compared to other DOT'S WA requires different dedup columns since the 'payment amount total' is calculated post scraping \n",
    "    # and if minor changes such as 'engineer name' or 'project description' occur individually that will not be captured as it would lead to erroneous payment amount total. However if these changes happen in tandem with \n",
    "    # any one of the columns identified for deduplication ('Contract_Number','Estimate_Number_Original','Payment_Amount','Payment_Date') then those changes will be captured\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Project_Cost_Total','Payment_Balance','Pull_Date_Initial','Project_Description',\n",
    "                                                                                     'Payment_Amount_Percent','Payment_Number','Payment_Total_Percent','Contractor_Name',\n",
    "                                                                                     'Payment_Amount_Total', 'Engineer_Name','Payment_Work_Period','Last_Date_of_Work_This_Payment'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Payment_Date'] = pd.to_datetime(combined_data['Payment_Date'], errors=\"coerce\")\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    # Sort the DataFrame\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Date\",\"Pull_Date_Initial\"], ascending=[True,True,False],na_position='first')\n",
    "    # Assign a payment estimate number\n",
    "    combined_data[\"Payment_Number\"] = combined_data.groupby(\"Contract_Number\").cumcount(ascending=True) \n",
    "    # Calculate the Current Payment Amount Total\n",
    "    combined_data['Payment_Amount_Total'] = combined_data[combined_data['Payment_Amount'] >= 0].groupby(\"Contract_Number\")['Payment_Amount'].cumsum()\n",
    "    # Revert the formatting of the date columns\n",
    "    combined_data[\"Payment_Date\"] = combined_data[\"Payment_Date\"].dt.strftime('%m/%d/%Y')\n",
    "    combined_data[\"Payment_Date\"] = combined_data[\"Payment_Date\"].fillna(\"N/A\")\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Calculate Project_Cost_Total by populating the the most recent Payment_Amount_Total. Note that this approach doesn't yield the 'True' Project value.\n",
    "    combined_data['Project_Cost_Total'] = combined_data.groupby('Contract_Number')['Payment_Amount_Total'].transform('max')\n",
    "    # Calculate Payment_Balance\n",
    "    combined_data['Payment_Balance'] = combined_data['Project_Cost_Total'] - combined_data['Payment_Amount_Total']\n",
    "    # Calculate Payment_Total_Percent\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]    \n",
    "\n",
    "else:\n",
    "    combined_data = scraped_data.copy()\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Estimate_Number_Original'] = combined_data['Estimate_Number_Original'].str.strip()\n",
    "    combined_data['Payment_Date'] = pd.to_datetime(combined_data['Payment_Date'], errors=\"coerce\")\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Date\",\"Pull_Date_Initial\"], ascending=[True,True,False],na_position='first')\n",
    "    # Assign a payment estimate number\n",
    "    combined_data[\"Payment_Number\"] = combined_data.groupby(\"Contract_Number\").cumcount(ascending=True)\n",
    "    # Calculate the Current Payment Amount Total\n",
    "    combined_data['Payment_Amount_Total'] = combined_data[combined_data['Payment_Amount'] >= 0].groupby(\"Contract_Number\")['Payment_Amount'].cumsum()\n",
    "    # Revert the formatting of the date columns\n",
    "    combined_data[\"Payment_Date\"] = combined_data[\"Payment_Date\"].dt.strftime('%m/%d/%Y')\n",
    "    combined_data[\"Payment_Date\"] = combined_data[\"Payment_Date\"].fillna(\"N/A\")\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Calculate Project_Cost_Total by populating the the most recent Payment_Amount_Total. Note that this approach doesn't yield the 'True' Project value.\n",
    "    combined_data['Project_Cost_Total'] = combined_data.groupby('Contract_Number')['Payment_Amount_Total'].transform('max')\n",
    "    # Calculate Payment_Balance\n",
    "    combined_data['Payment_Balance'] = combined_data['Project_Cost_Total'] - combined_data['Payment_Amount_Total']\n",
    "    # Calculate Payment_Total_Percent\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"Washington scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'Washington scraping completed and DUCKDB file updated Successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(\"Washington\", \"data_store_WA\", \"WA_DOT\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c9187",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Washington\\Monthly\\wa_bulk_pipeline_april_updated.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329f275e",
   "metadata": {},
   "source": [
    "Illinois Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa50c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de Logging\n",
    "logging.basicConfig(\n",
    "    filename='Illinois_scraping.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
    ")\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "SMTP_PORT = 587  # Usually 587 for TLS\n",
    "EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "def send_email(subject, body):\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = EMAIL_SENDER\n",
    "        msg['To'] = EMAIL_RECIPIENT\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        logging.info(\"Email notification sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email notification: {e}\")\n",
    "        logging.error(f\"Failed to send email notification: {e}\")\n",
    "        \n",
    "# Retrieve contract numbers\n",
    "\n",
    "# WebDriver Manager will handle everything automatically\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# URL of the site\n",
    "url = 'https://apps1.dot.illinois.gov/WCP/PEstimate/PEstimateSearch'\n",
    "\n",
    "contract_numbers_il = []\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "driver.maximize_window() # resolves issue where dropdown buttons on this page are not visible when window is minimized\n",
    "\n",
    "try:\n",
    "    time.sleep(10)\n",
    "    wait = WebDriverWait(driver, 20)\n",
    "    contract_num_button = wait.until(\n",
    "        EC.visibility_of_element_located((By.XPATH, '/html/body/div[1]/div[2]/div[1]/div/div/form[2]/div/div[2]/span/button'))\n",
    "    )\n",
    "    contract_num_button.click()\n",
    "\n",
    "    contract_num_list = wait.until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, '/html/body/div[4]/div/div/div[2]/ul/li'))\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(contract_num_list):\n",
    "        number = row.get_attribute(\"textContent\")\n",
    "        contract_numbers_il.append(number)\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "\n",
    "# Get associated Contract information\n",
    "driver = webdriver.Chrome(service=service)\n",
    "# URL of the site\n",
    "url = 'https://apps1.dot.illinois.gov/WCP/'\n",
    "\n",
    "\n",
    "# Create connection to duckdb table\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Illinois\\data_store_IL.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM IL_DOT\").df()\n",
    "con.close()\n",
    "df1 = existing_data.drop_duplicates(subset=[\"Contract_Number\"], keep='first')\n",
    "\n",
    "# Convert 'Payment_Date' to datetime format\n",
    "df1['Payment_Date'] = pd.to_datetime(df1['Payment_Date'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Convert 'Percent_Complete' to numeric format (remove '%' and convert to float)\n",
    "df1['Payment_Total_Percent'] = df1['Payment_Total_Percent'].str.rstrip('%').astype(float)\n",
    "\n",
    "# Define the filter conditions\n",
    "condition1 = (df1['Payment_Date'].dt.year <= 2024) & (df1['Payment_Total_Percent'] >= 100)\n",
    "condition2 = (df1['Payment_Date'].dt.year <= 2023) & (df1['Payment_Total_Percent'] >= 95)\n",
    "condition3 = df1['Payment_Date'].dt.year <= 2021\n",
    "\n",
    "# Apply filtering: Keep only rows that satisfy any of the conditions\n",
    "filtered_df = df1[(condition1 | condition2 | condition3)]\n",
    "inactive_contracts = filtered_df['Contract_Number'].to_list()\n",
    "# Filter out inactive contracts for scraping\n",
    "contract_numbers_il_updated = [num for num in contract_numbers_il if num not in inactive_contracts]\n",
    "print(len(contract_numbers_il_updated))\n",
    "\n",
    "# Contract number to search for\n",
    "contract_numbers = contract_numbers_il_updated\n",
    "\n",
    "# Start a new browser session\n",
    "driver.get(url)\n",
    "driver.maximize_window() \n",
    "row_data_list_il = []\n",
    "header_data_il = [\"percent_completed\", \"current_report\",\"from_date\", \"to_date\", \"Contract_Awarded_Amount\", \"Additions\",\n",
    "                  \"Deductions\", \"Total_Adjusted_Contract_Value\", \"Total_Amount_Due_to_Date\", \"Payment_Number\", \"total\", \"contract_number\",\n",
    "                  \"state_job\", \"dot_vendor\", \"district/county\", \"il_project\", \"letting_date\", \"route\", \"airport/section\", \"section\",\n",
    "                  \"project\", \"payee\", \"scope\"]\n",
    "\n",
    "try:\n",
    "    wait = WebDriverWait(driver, 100)\n",
    "    for i,value in enumerate(contract_numbers):\n",
    "        # Wait for the contract number input box to be present\n",
    "        state_no_input = WebDriverWait(driver, 15).until(     \n",
    "            EC.visibility_of_element_located((By.ID, 'ContractNumber')) \n",
    "        ) \n",
    "        state_no_input.send_keys(value)\n",
    "        state_no_input.send_keys(Keys.RETURN)\n",
    "        try:\n",
    "            try:\n",
    "                contract_number_link = WebDriverWait(driver, 7).until(\n",
    "                    EC.presence_of_element_located((By.LINK_TEXT, value))\n",
    "                )\n",
    "                time.sleep(3) # possibly resolves staleelement error\n",
    "                contract_number_link.click()\n",
    "            except StaleElementReferenceException:\n",
    "                print(value,\"StaleElementReferenceException error. Rescrape\")\n",
    "                continue\n",
    "            contract_info = []\n",
    "            contractor_info = WebDriverWait(driver,10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"EstimateChoiceContractorInfo\"]'))\n",
    "            )\n",
    "            time.sleep(5)\n",
    "            all_invoices = WebDriverWait(driver,15).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, \"/html/body/div[1]/div[2]/div[1]/div[3]/div/div/div[1]/ul/li/span[@class='k-link']\"))\n",
    "            )\n",
    "            \n",
    "            data = contractor_info.find_elements(By.XPATH, './/div[label]')\n",
    "\n",
    "            if len(data) != 12:\n",
    "                print(\"possible data column mismatch at following contract and iteration\", value,i)\n",
    "\n",
    "            for info in data:\n",
    "                cell = info.find_element(By.XPATH, \"./div\")\n",
    "                contract_info.append(cell.get_attribute(\"textContent\").strip())\n",
    "    \n",
    "            # iterate through all transactions\n",
    "            report_counter = 0\n",
    "            try:\n",
    "                for j, invoice in enumerate(all_invoices):\n",
    "                    if report_counter > 2: # fetch latest 3 reports only since historical data has been scraped already\n",
    "                        break\n",
    "                    print(\"current value, iteration, and invoice\", value, i, j)\n",
    "                    if j == 0: # This condition resolves ElementClickInterceptedException error which can occur during first iteration\n",
    "                        WebDriverWait(driver, 25).until(\n",
    "                        EC.visibility_of_element_located((By.XPATH, './/div[@ID=\"TabStrip\"]'))\n",
    "                        )\n",
    "                        current_id = \"TabStrip-\" + str(j+1)\n",
    "                        container = wait.until(\n",
    "                            EC.visibility_of_element_located((By.ID, current_id)) # modify this to remove sleep(add class container)\n",
    "                        )\n",
    "                    invoice.click()\n",
    "                    time.sleep(1)   \n",
    "                    # Wait for content to load\n",
    "                    if j > 0:\n",
    "                        wait.until(\n",
    "                            EC.visibility_of_element_located((By.XPATH, './/div[@ID=\"TabStrip\"]')) # modify this to remove sleep(add class container)\n",
    "                        )\n",
    "                        current_id = \"TabStrip-\" + str(j+1)\n",
    "                        container = wait.until(\n",
    "                            EC.visibility_of_element_located((By.ID, current_id)) # modify this to remove sleep(add class container)\n",
    "                        )\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # Retrieve dates and percent complete information\n",
    "                    current_data = []\n",
    "                    payment_data1 = container.find_element(By.XPATH, './/div[@class=\"container mt-3\"]')\n",
    "                    for cell in payment_data1.find_elements(By.XPATH,'.//div[@class=\"form-group\"]/div'):\n",
    "                        current_data.append(cell.get_attribute(\"textContent\").strip())\n",
    "\n",
    "                    # Retrieve additional contract payment info such as 'contract awarded amount', 'additions', 'Total Adjusted Contract Value' and more\n",
    "                    payment_data2 = container.find_element(By.XPATH, './div[3]')\n",
    "                    payment_data2 = payment_data2.find_element(By.XPATH, './/table[1]/tbody/tr')\n",
    "                    for cell in payment_data2.find_elements(By.XPATH, './td'):\n",
    "                        current_data.append(cell.get_attribute(\"textContent\").strip())\n",
    "\n",
    "                    # Retrieve Total Cost and payment number of corresponding invoice\n",
    "                    payment_number = container.find_element(By.XPATH, \"./div[3]//div[@class='form-group'][last()]/div[@class='form-group row']/div[2]\")\n",
    "                    Total = container.find_element(By.XPATH, \"./div[3]//div[@class='form-group'][last()]/div[@class='form-group row']/div[last()]\")\n",
    "                    current_data.append(payment_number.text)\n",
    "                    current_data.append(Total.text)\n",
    "                    combined_data = current_data + contract_info\n",
    "                    row_data_list_il.extend([combined_data])      \n",
    "                    report_counter += 1\n",
    "\n",
    "            except ElementClickInterceptedException:\n",
    "                print(\"ElementClickInterceptedException error, rescrape\", value)\n",
    "            except TimeoutException:        \n",
    "                print(value,\"Invoice loading timed out\")\n",
    "            except StaleElementReferenceException:\n",
    "                print(value,\"Loading Error\")\n",
    "\n",
    "        except TimeoutException:        \n",
    "            print(value,\"Not Found\")\n",
    "                \n",
    "        driver.get(url)\n",
    "\n",
    "# Close the browser\n",
    "finally:\n",
    "    driver.quit()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ddd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DuckDB Integration\n",
    "il_dot_data = pd.DataFrame(data=row_data_list_il, columns = header_data_il)\n",
    "del il_dot_data['section']\n",
    "del il_dot_data['il_project']\n",
    "\n",
    "# POST PROCESSING\n",
    "df = il_dot_data.copy()\n",
    "df = df[df['Payment_Number'].str.isnumeric()]\n",
    "df['Payment_Number'] = df['Payment_Number'].astype(int)\n",
    "# Split \"district/county\" into 'Project_Location_District' and 'Project_Location_County'\n",
    "df[['Project_Location_District', 'Project_Location_County']] = df['district/county'].str.extract(r'([\\w\\s]+?)\\s*-\\s*\\d{3}\\s*\\(\\s*([\\w\\s.]+?)\\s*\\)')\n",
    "del df['district/county']\n",
    "\n",
    "# Create payment work period column from \"from_date\" and \"to_date\"\n",
    "df[\"Payment_Work_Period\"] = df[\"from_date\"].str.cat(df[\"to_date\"], sep=\" to \", na_rep=\"Unknown\")\n",
    "del df['from_date']\n",
    "del df['to_date']\n",
    "\n",
    "df.rename(columns = {'project':'Project_Number','scope':'Project_Description','route':'Project_Location_Route','airport/section':'Project_Location_Section',\n",
    "                     'state_job':'Project_Location_StateJob','letting_date':'Project_Letting','contract_number':'Contract_Number','dot_vendor':'Contractor_DOT',\n",
    "                     'total':'Payment_Amount','current_report':'Payment_Date','payee':'Contractor_Name','percent_completed':'Payment_Total_Percent',\n",
    "                     'Total_Amount_Due_to_Date':'Payment_Amount_Total','Contract_Awarded_Amount':'Project_Cost_Total','Additions':'Project_Cost_Additions',\n",
    "                     'Deductions':'Project_Cost_Deductions','Total_Adjusted_Contract_Value':'Project_Cost_Total_Adjusted'}, inplace=True)\n",
    "def parse_money(value):\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value) if value.strip() else None \n",
    "\n",
    "# Convert money columns into calculable columns\n",
    "df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "df['Project_Cost_Additions'] = df['Project_Cost_Additions'].apply(parse_money)\n",
    "df['Project_Cost_Deductions'] = df['Project_Cost_Deductions'].apply(parse_money)\n",
    "df['Project_Cost_Total_Adjusted'] = df['Project_Cost_Total_Adjusted'].apply(parse_money)\n",
    "\n",
    "df['Payment_Balance'] = (df['Project_Cost_Total_Adjusted'] - df['Payment_Amount_Total']).round(2)\n",
    "\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Illinois\\data_store_IL.duckdb\"\n",
    "table_name = \"IL_DOT\"\n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df.copy()\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Contract_Number  TEXT,\n",
    "Contractor_Name  TEXT,\n",
    "Contractor_DOT  TEXT,\n",
    "Project_Number  TEXT,\n",
    "Payment_Number  INTEGER,\n",
    "Project_Description\tTEXT,\n",
    "Project_Location_Route TEXT,\n",
    "Project_Location_Section  TEXT,\n",
    "Project_Location_StateJob  TEXT,\n",
    "Project_Location_District  TEXT, \n",
    "Project_Location_County  TEXT,\n",
    "Project_Letting  TEXT,\n",
    "Payment_Date  TEXT,\n",
    "Payment_Work_Period  TEXT,\n",
    "\n",
    "Payment_Amount DOUBLE,\t\n",
    "Project_Cost_Total  DOUBLE,\t\n",
    "Project_Cost_Additions  DOUBLE,\n",
    "Project_Cost_Deductions  DOUBLE,\n",
    "Project_Cost_Total_Adjusted  DOUBLE,\n",
    "Payment_Amount_Total  DOUBLE,\n",
    "Payment_Total_Percent  TEXT,\n",
    "Payment_Balance  DOUBLE,\n",
    "Pull_Date_Initial TEXT,\n",
    "Payment_Amount_Percent FLOAT,\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    # find duplicates by all columns except the columns below since they are calculated later and those columns that need not be tracked for changes if occur separately.\n",
    "    # eg: for a given contract and payment number, if only \"Contractor_DOT\" is changed while other fields are the same we don't need to capture that unless required by Census.\n",
    "    # Later, a logic can be developed to find revised payments if any\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial', 'Payment_Amount_Percent','Contractor_DOT','Project_Location_District','Project_Location_County','Payment_Balance'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Number\",\"Pull_Date_Initial\"], ascending=[True,False,False])\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "else:\n",
    "    combined_data = scraped_data.copy()\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Contract_Number\", \"Payment_Number\",\"Pull_Date_Initial\"], ascending=[True,False,False])\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"Illinois scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'Illinois scraping completed and DUCKDB file updated Successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264fc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"Illinois\", \"data_store_IL\", \"IL_DOT\")\n",
    "data.head()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Illinois\\Monthly\\il_bulk_pipeline_april_2025.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba88d17b",
   "metadata": {},
   "source": [
    "Delaware Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80844c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all contract numbers\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "# URL of the site\n",
    "url = 'https://deldot.gov/Business/ProjectStatusQuery/'\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "contract_numbers_del = []\n",
    "header_contract_del = [\"contract_number\",\"awarded_date\"]\n",
    "\n",
    "driver.maximize_window()\n",
    "time.sleep(12) # Next button is disabled initially while page is loading. Explicit wait is required\n",
    "try:\n",
    "    next_page_button = WebDriverWait(driver, 20).until(\n",
    "            EC.visibility_of_element_located((By.XPATH, '//*[@id=\"property_next\"]'))\n",
    "            )        \n",
    "    while \"disabled\" not in next_page_button.get_attribute(\"class\"): \n",
    "    # wait to load\n",
    "        body = WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"property\"]'))\n",
    "                )\n",
    "        WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"property\"]/tbody/tr[1]/td[2]/a'))\n",
    "                )\n",
    "        contract_numbers = driver.find_elements(By.XPATH, '//*[@id=\"property\"]/tbody/tr')\n",
    "        for row in contract_numbers:\n",
    "            current_contract = []\n",
    "            cell_2 = row.find_element(By.XPATH, \"./td[2]/a\")\n",
    "            cell_5 = row.find_element(By.XPATH, \"./td[5]\")\n",
    "            current_contract.append(cell_2.text)\n",
    "            current_contract.append(cell_5.text)\n",
    "            # if len(current_contract) < 2:\n",
    "            #     current_contract.append(\" \")\n",
    "            contract_numbers_del.append(current_contract)\n",
    "        next_page_button.click()\n",
    "        next_page_button = WebDriverWait(driver, 20).until(\n",
    "        EC.visibility_of_element_located((By.XPATH, '//*[@id=\"property_next\"]'))\n",
    "        ) \n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "del_contracts = pd.DataFrame(data=contract_numbers_del, columns = header_contract_del)\n",
    "del_contracts.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out inactive contracts\n",
    "df_updated = del_contracts[del_contracts['awarded_date'] != '']\n",
    "contract_numbers_del_filtered = df_updated['contract_number'].to_list()\n",
    "\n",
    "# Filter out completed contracts\n",
    "# Create connection to duckdb table\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Delaware\\data_store_dl.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM DL_DOT\").df()\n",
    "con.close()\n",
    "df1 = existing_data.drop_duplicates(subset=[\"Project_Number\"], keep='last')\n",
    "print(df1.shape)\n",
    "print(len(df1['Project_Number'].unique()))\n",
    "\n",
    "# Convert 'Payment_Date' to datetime format\n",
    "df1['Payment_Date'] = pd.to_datetime(df1['Payment_Date'], format='%m/%d/%Y', errors='coerce')\n",
    "df1['Project_Bid_Awarded'] = pd.to_datetime(df1['Project_Bid_Awarded'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Define the filter conditions\n",
    "condition1 = (df1['Payment_Date'].dt.year <= 2024) & (df1['Payment_Total_Percent'] >= 100)\n",
    "condition2 = (df1['Payment_Date'].dt.year <= 2023) & (df1['Payment_Total_Percent'] >= 95)\n",
    "condition3 = df1['Payment_Date'].dt.year <= 2021\n",
    "condition4 = df1['Payment_Status'] == \"Final\"\n",
    "condition5 = (df1['Project_Bid_Awarded'].dt.year <= 2010)\n",
    "\n",
    "# Apply filtering: Keep only rows that satisfy any of the conditions\n",
    "filtered_df = df1[(condition1 | condition2 | condition3 | condition4 | condition5)]\n",
    "inactive_contracts_dl = filtered_df['Project_Number'].to_list()\n",
    "# Filter out inactive contracts for scraping\n",
    "contract_numbers_dl_updated = [num for num in contract_numbers_del_filtered if num not in inactive_contracts_dl]\n",
    "print(len(contract_numbers_dl_updated))\n",
    "\n",
    "# Contract number to search for\n",
    "contract_numbers = contract_numbers_dl_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# URL of the site\n",
    "url = 'https://deldot.gov/Business/ProjectStatusQuery/'\n",
    "                    \n",
    "contract_numbers = contract_numbers_dl_updated\n",
    "# contract_numbers = [\"T200507103\"]\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "row_data_list_del = []\n",
    "header_data_del = []\n",
    "original_window = driver.window_handles[0]\n",
    "\n",
    "def extract_text(cell):\n",
    "                    \"\"\"Extracts text from a cell, replacing <br> with new lines.\"\"\"\n",
    "                    return cell.get_attribute('innerHTML').replace('<br>', '\\n').strip()\n",
    "try:\n",
    "    for t,value in enumerate(contract_numbers):\n",
    "        print(f\"iteration {t}th, working on {value}\")\n",
    "        # Wait for the contract number input box to be present\n",
    "        state_no_input = WebDriverWait(driver, 15).until(     \n",
    "            EC.visibility_of_element_located((By.CSS_SELECTOR, '#stateNoFilter input')) \n",
    "        ) \n",
    "        state_no_input.send_keys(value)\n",
    "        try:\n",
    "            contract_number_link = WebDriverWait(driver, 70).until(\n",
    "                EC.presence_of_element_located((By.LINK_TEXT, value))\n",
    "            )\n",
    "        \n",
    "            contract_number_link.click()\n",
    "        \n",
    "            # Find tables\n",
    "            info_table2 = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"project-info\"]/div/table[2]/tbody/tr'))\n",
    "            )\n",
    "            info_table1 = WebDriverWait(driver, 15).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"project-info\"]/div/table[1]/tbody/tr'))\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "            # Extract the table headers\n",
    "                if t == 0 or not header_data_del:\n",
    "                    payment_header = driver.find_element(By.XPATH, '//table[@id=\"paymentInfo\"]/thead/tr').find_elements(By.XPATH,'.//th') \n",
    "                    for h,header in enumerate(payment_header):\n",
    "                        if h != 1 and h != 2:\n",
    "                            header_data_del.append(header.get_attribute(\"textContent\").strip())\n",
    "                        else:\n",
    "                            continue\n",
    "                    header_data_del = ['project_number'] + ['description'] + ['justification'] + ['advertise_date'] + ['bid_amount'] + ['bids_received_date'] + ['date_awarded'] + ['total_project_cost'] + ['first_chargeable_day'] + ['contractor'] + ['estimated_completion_date'] +['company_name']+['address']+['attorney_name']+['attorney_company_name'] + ['attorney_address'] + ['attorney_phone_num'] + ['bond_number'] + header_data_del+['Estimated_Cost_of_Final_Contract'] + ['Total_Estimate_to_Date'] + ['Contract_Award_Price'] + ['Auth_Retainage'] + ['Proposed_Time']+['Held_in_Securities'] + ['Time_Extended'] + ['Amt_to_be_Retained'] + ['Total_Time'] + ['Previous_Payment'] + ['Previous_time_charged']+['Liq_Damages_($/Day)'] + ['Time_Used_this_period'] + ['Time_Remaining'] + ['Misc_Deduction'] + ['Incentive_Prg(+/-)'] + ['Total_Deduction']+['Amount_this_Estimate'] # delete 'Amount_this_Estimate' later as it is same as 'Est Amt'\n",
    "            except NoSuchElementException:\n",
    "                print(value,\"Out of scope/invalid format. Re-check periodically\") \n",
    "                driver.get(url)\n",
    "                continue\n",
    "               \n",
    "            # Extract Total Project Cost\n",
    "            contract_info = []\n",
    "            contract_info.append(value)\n",
    "            for j, info in enumerate(info_table1):\n",
    "                cells = info.find_elements(By.XPATH,'./td') \n",
    "                if j == 1 or j == 2:\n",
    "                    contract_info.append(cells[1].get_attribute('textContent'))\n",
    "\n",
    "            for c, row in enumerate(info_table2):\n",
    "                cells = row.find_elements(By.XPATH,'./td') \n",
    "                if c == len(info_table2) - 1 or c == 1:\n",
    "                    contract_info.append(cells[1].get_attribute('textContent'))\n",
    "                else: \n",
    "                    contract_info.append(cells[1].get_attribute('textContent'))\n",
    "                    contract_info.append(cells[3].get_attribute('textContent'))\n",
    "            try:\n",
    "                info_table_bond = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"bond-info\"]/div/table/tbody/tr'))\n",
    "                )\n",
    "                for k, row in enumerate(info_table_bond):\n",
    "                    cells = row.find_elements(By.XPATH,'./td')\n",
    "                    if k == len(info_table_bond) - 1:\n",
    "                        contract_info.append(extract_text(cells[1]))\n",
    "                    else:\n",
    "                        contract_info.append(extract_text(cells[1]))\n",
    "                        contract_info.append(extract_text(cells[3]))\n",
    "            except TimeoutException:\n",
    "                 # Append empty values when bond information is not available to avoid assertion errors while creating data frames\n",
    "                 for i in range(7):\n",
    "                    contract_info.append(\" \")    \n",
    "                 print(\"No Bond Information\",value)\n",
    "            \n",
    "\n",
    "            # if len(contract_info) != 17:\n",
    "            #     print(\"check contract number data\", value, i)\n",
    "            time.sleep(5) # wait for transactions to load\n",
    "            # Extract current amount\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//table[@ID=\"paymentInfo\"]/tbody/tr'))\n",
    "            )\n",
    "            payment_data = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//table[@ID=\"paymentInfo\"]/tbody/tr'))\n",
    "            )\n",
    "            for row in reversed(payment_data[-3:]): # Pull in the last 3 reports only\n",
    "                current_row = []\n",
    "                row_data = WebDriverWait(row, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, './td'))\n",
    "                )\n",
    "                if len(payment_data) <= 1 and len(row_data) <= 1:\n",
    "                    current_row.extend(contract_info)\n",
    "                    current_row.append(\"No Payment History Available\")\n",
    "                    row_data_list_del.append(current_row)\n",
    "                    break\n",
    "\n",
    "                current_row.extend(contract_info)\n",
    "                # current_row.extend([cell.get_attribute(\"textContent\") for cell in row_data if cell.get_attribute(\"textContent\")])\n",
    "                for i, data in enumerate(row_data):\n",
    "                    if i == 1:\n",
    "                        header_detail_link = data.find_element(By.XPATH,'./a')\n",
    "                current_row.extend([cell.text for cell in row_data])\n",
    "                del current_row[19:21]\n",
    "                header_detail_link.send_keys(Keys.CONTROL + Keys.RETURN)\n",
    "                \n",
    "                # change the window of the driver since new tab is opened\n",
    "                new_window_handle = [handle for handle in driver.window_handles if handle != original_window][-1]\n",
    "                driver.switch_to.window(new_window_handle)\n",
    "                time.sleep(0.5) # wait for page content to be loaded properly\n",
    "                header_detail_info = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"project-info\"]/div/table/tbody/tr'))\n",
    "                )\n",
    "                for k, detail in enumerate(header_detail_info):\n",
    "                    cell_data = detail.find_elements(By.XPATH,'./td')\n",
    "                    # Below conditions ensure inconsistent arrangement of data elements is handled\n",
    "                    if k < 8 and k != 6:\n",
    "                        # Extract only the values not the headers\n",
    "                        current_row.append(cell_data[1].get_attribute('textContent'))\n",
    "                        current_row.append(cell_data[3].get_attribute('textContent'))\n",
    "                    elif k == 6:\n",
    "                        current_row.append(str(cell_data[1].get_attribute('textContent')) + \"-\" + str(cell_data[2].get_attribute('textContent')))\n",
    "                    else:\n",
    "                        current_row.append(cell_data[3].get_attribute('textContent'))\n",
    "\n",
    "                # Go back to details page\n",
    "                # details_page = WebDriverWait(driver, 10).until(\n",
    "                # EC.presence_of_element_located((By.LINK_TEXT, '[Back to Details Page]'))\n",
    "                # )\n",
    "                row_data_list_del.append(current_row)\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "                # details_page.click() # go back to all estimates\n",
    "                \n",
    "                      \n",
    "        except TimeoutException:        \n",
    "            print([value,t],\"Not Found\")\n",
    "\n",
    "        # except StaleElementReferenceException:\n",
    "        #     print(value,\"Loading Error\")\n",
    "                \n",
    "        driver.get(url)\n",
    "\n",
    "       \n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "del_dot_data = pd.DataFrame(data=row_data_list_del, columns = header_data_del)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbed15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "SMTP_PORT = 587  # Usually 587 for TLS\n",
    "EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "def send_email(subject, body):\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = EMAIL_SENDER\n",
    "        msg['To'] = EMAIL_RECIPIENT\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        logging.info(\"Email notification sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email notification: {e}\")\n",
    "        logging.error(f\"Failed to send email notification: {e}\")\n",
    "\n",
    "# POST PROCESSING\n",
    "df = del_dot_data.copy()\n",
    "del df['Amount_this_Estimate']\n",
    "\n",
    "df.rename(columns = {'project_number':'Project_Number','description':'Project_Description','justification':'Project_Justification','advertise_date':'Project_Advertise',\n",
    "                     'bid_amount':'Project_Bid_Amount','bids_received_date':'Project_Bid_Rcvd','date_awarded':'Project_Bid_Awarded','total_project_cost':'Project_Cost_Total',\n",
    "                     'first_chargeable_day':'Project_Start_Actual','contractor':'Contractor_Name','estimated_completion_date':'Project_Comp_Est','bond_number':'Bond_Number',\n",
    "                     'company_name':'Bond_Company_Name','address':'Bond_Company_Address','attorney_name':'Bond_Attorney_Contact','attorney_company_name':'Bond_Attorney_Name',\n",
    "                     'attorney_address':'Bond_Attorney_Address','attorney_phone_num':'Bond_Attorney_Phone','Est No':'Payment_Number','Status':'Payment_Status','Est Amt':'Payment_Amount',\n",
    "                     'Payment Type':'Payment_Type','Advice No':'Payment_CheckNo','Advice Date':'Payment_Date','Total_Estimate_to_Date':'Payment_Amount_Total',\n",
    "                     'Liq_Damages_($/Day)':'Liq_Damages_Per_Day','Incentive_Prg(+/-)':'Incentive_Prg_pos_or_neg'\n",
    "                    }, inplace=True)\n",
    "\n",
    "def parse_money(value):\n",
    "    if not value:  # Check for None, empty string, or NaN\n",
    "        return None\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value) if value.strip() else None\n",
    "\n",
    "df['Project_Cost_Total'] = df['Project_Cost_Total'].apply(parse_money)\n",
    "df['Payment_Amount_Total'] = df['Payment_Amount_Total'].apply(parse_money)\n",
    "df['Project_Bid_Amount'] = df['Project_Bid_Amount'].apply(parse_money)\n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "df['Estimated_Cost_of_Final_Contract'] = df['Estimated_Cost_of_Final_Contract'].apply(parse_money)\n",
    "df['Contract_Award_Price'] = df['Contract_Award_Price'].apply(parse_money)\n",
    "df['Auth_Retainage'] = df['Auth_Retainage'].apply(parse_money)\n",
    "df['Held_in_Securities'] = df['Held_in_Securities'].apply(parse_money)\n",
    "df['Amt_to_be_Retained'] = df['Amt_to_be_Retained'].apply(parse_money)\n",
    "df['Liq_Damages_Per_Day'] = df['Liq_Damages_Per_Day'].apply(parse_money)\n",
    "df['Previous_Payment'] = df['Previous_Payment'].apply(parse_money)\n",
    "df['Misc_Deduction'] = df['Misc_Deduction'].apply(parse_money)\n",
    "df['Incentive_Prg_pos_or_neg'] = df['Incentive_Prg_pos_or_neg'].apply(parse_money)\n",
    "df['Total_Deduction'] = df['Total_Deduction'].apply(parse_money)\n",
    "\n",
    "df = df[df['Payment_Number'].str.isnumeric()]\n",
    "df['Payment_Number'] = df['Payment_Number'].astype(int)\n",
    "# Calculate Payment Balance\n",
    "df['Payment_Balance'] = df['Project_Cost_Total'] - df['Payment_Amount_Total']\n",
    "\n",
    "# Function to extract components\n",
    "def extract_address_components(address):\n",
    "    lines = address.split(\"\\n\")  # Split into lines\n",
    "    \n",
    "    if len(lines) < 2:\n",
    "        return pd.NA, pd.NA, pd.NA, pd.NA  # Handle unexpected formats\n",
    "    \n",
    "    city_state_zip = lines[-1]  # Last line contains City, State, Zip\n",
    "    parts = city_state_zip.split()  # Split by spaces\n",
    "\n",
    "    if len(parts) < 3:  # Need at least City, State, Zip\n",
    "        print(f\"Malformed address: {address}\")\n",
    "        return pd.NA, pd.NA, pd.NA, pd.NA\n",
    "\n",
    "    zipcode = parts[-1]  # Last part is ZIP code\n",
    "    state = parts[-2]  # Second last is state\n",
    "    city = \" \".join(parts[:-2])  # Everything before state is city\n",
    "    \n",
    "    street = \" \".join(lines[:-1])  # Everything before last line is street\n",
    "\n",
    "    return street, city, state, zipcode\n",
    "\n",
    "# Apply function to DataFrame\n",
    "df[['Bond_Company_Street', 'Bond_Company_City', 'Bond_Company_State', 'Bond_Company_Zipcode']] = df['Bond_Company_Address'].apply(\n",
    "    lambda x: pd.Series(extract_address_components(x))\n",
    ")\n",
    "df[['Bond_Attorney_Street', 'Bond_Attorney_City', 'Bond_Attorney_State', 'Bond_Attorney_Zipcode']] = df['Bond_Attorney_Address'].apply(\n",
    "    lambda x: pd.Series(extract_address_components(x))\n",
    ")\n",
    "\n",
    "# Strip all leading and trailing whitespaces in all the string columns of a dataframe, before using drop_suplicates()\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Delaware\\data_store_DL.duckdb\"\n",
    "table_name = \"DL_DOT\"\n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df\n",
    "print(df.columns)\n",
    "print(len(df.columns))\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Contractor_Name  TEXT,\n",
    "Project_Number  TEXT,\n",
    "Payment_Number  INTEGER,\n",
    "Project_Description\tTEXT,\n",
    "Project_Justification  TEXT,\n",
    "Project_Comp_Est  TEXT,\n",
    "Project_Advertise  TEXT,\n",
    "Project_Bid_Rcvd  TEXT,\t\n",
    "Project_Bid_Awarded  TEXT,\t\n",
    "Project_Start_Actual  TEXT,\n",
    "\n",
    "Bond_Number  TEXT,\n",
    "Bond_Company_Name  TEXT,\n",
    "Bond_Company_Address  TEXT,\n",
    "Bond_Attorney_Contact  TEXT,\n",
    "Bond_Attorney_Name  TEXT,\n",
    "Bond_Attorney_Address  TEXT,\n",
    "Bond_Attorney_Phone  TEXT,\n",
    "Bond_Company_Street  TEXT, \n",
    "Bond_Company_City  TEXT, \n",
    "Bond_Company_State  TEXT, \n",
    "Bond_Company_Zipcode  TEXT,\n",
    "Bond_Attorney_Street  TEXT, \n",
    "Bond_Attorney_City  TEXT, \n",
    "Bond_Attorney_State  TEXT, \n",
    "Bond_Attorney_Zipcode  TEXT,\n",
    "\n",
    "Project_Bid_Amount  DOUBLE,\n",
    "Payment_Amount DOUBLE,\n",
    "Payment_Status  TEXT,\n",
    "Payment_Type  TEXT,\n",
    "Payment_CheckNo  TEXT,\n",
    "Payment_Date  TEXT,\t\n",
    "Project_Cost_Total  DOUBLE,\t\n",
    "Payment_Amount_Total  DOUBLE,\n",
    "Payment_Total_Percent  FLOAT,\n",
    "Payment_Balance  DOUBLE,\n",
    "Pull_Date_Initial TEXT,\n",
    "Payment_Amount_Percent FLOAT,\n",
    "\n",
    "Estimated_Cost_of_Final_Contract  DOUBLE,\n",
    "Contract_Award_Price  DOUBLE, \n",
    "Auth_Retainage  DOUBLE,\n",
    "Proposed_Time  TEXT, \n",
    "Held_in_Securities  DOUBLE, \n",
    "Time_Extended  TEXT,\n",
    "Amt_to_be_Retained  DOUBLE, \n",
    "Total_Time  TEXT, \n",
    "Previous_Payment  DOUBLE,\n",
    "Previous_time_charged  TEXT, \n",
    "Liq_Damages_Per_Day  DOUBLE, \n",
    "Time_Used_this_period  TEXT,\n",
    "Time_Remaining  TEXT, \n",
    "Misc_Deduction  DOUBLE, \n",
    "Incentive_Prg_pos_or_neg  DOUBLE,\n",
    "Total_Deduction  DOUBLE\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "print(existing_data.columns)\n",
    "print(len(existing_data.columns))\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial', 'Payment_Amount_Percent', 'Payment_Total_Percent'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Project_Number\",\"Payment_Number\", \"Pull_Date_Initial\"], ascending=[True,True,False])\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "else:\n",
    "    combined_data = scraped_data\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Pull_Date_Initial\"], ascending=[True,False])\n",
    "    # Revert the formatting of pull_date_initial column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]\n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"Delaware scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'Delaware scraping completed and DUCKDB file updated Successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad043849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"Delaware\", \"data_store_DL\", \"DL_DOT\")\n",
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Delaware\\Monthly\\de_bulk_pipeline_april.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf6dd33",
   "metadata": {},
   "source": [
    "LA City Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158d889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of contract numbers with add info. Will be used as input in next step\n",
    "\n",
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# URL of the site\n",
    "url = 'https://bca.lacity.gov/approvedsubs/'          \n",
    "\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "row_data_list_la_sub = []\n",
    "header_data_la_sub = [\"work_order\",\"project_title\",\"subcontractor_name\",\"work_description\",\"work_value\"]\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"company_wrapper\"))\n",
    "    )\n",
    "    # Submit the search form\n",
    "    try: \n",
    "        set_records = driver.find_element(By.XPATH, '//*[@id=\"company_length\"]/label/select/option[4]')\n",
    "        set_records.click()\n",
    "        table = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"company\"]'))\n",
    "        )\n",
    "            \n",
    "        # Iterate and extract work order no's & add info over all the pages\n",
    "        next_page_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"company_next\"]'))\n",
    "            ) \n",
    "              \n",
    "        while \"disabled\" not in next_page_button.get_attribute(\"class\"):\n",
    "            WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"company_wrapper\"))\n",
    "            )\n",
    "            WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"company\"]/tbody/tr'))\n",
    "            )\n",
    "            rows = driver.find_elements(By.XPATH, '//*[@id=\"company\"]/tbody/tr')\n",
    "            time.sleep(2)\n",
    "            for i,row in enumerate(rows):\n",
    "                row_data = row.find_elements(By.XPATH, './td')\n",
    "                row_data_list_la_sub.append([cell.get_attribute(\"textContent\") for cell in row_data[:-1]])   \n",
    "            next_page_button_link = driver.find_element(By.XPATH,'//*[@id=\"company_next\"]/a') \n",
    "            next_page_button_link.click()\n",
    "            time.sleep(2)\n",
    "            WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"company\"]'))\n",
    "            )             \n",
    "            next_page_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//*[@id=\"company_next\"]'))\n",
    "            )  \n",
    "            if \"disabled\" in next_page_button.get_attribute(\"class\"):\n",
    "                rows =  WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"company\"]/tbody/tr'))\n",
    "                )\n",
    "                for i,row in enumerate(rows):\n",
    "                    row_data = driver.find_elements(By.XPATH, './td')\n",
    "                    row_data_list_la_sub.extend([cell.get_attribute(\"textContent\") for cell in row_data[:1]])\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(value,\"Not Found\")\n",
    "        \n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "LA_city_data_sub = pd.DataFrame(data=row_data_list_la_sub, columns = header_data_la_sub)\n",
    "def parse_money(value):\n",
    "    if not value:  # Check for None, empty string, or NaN\n",
    "        return None\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value) if value.strip() else None\n",
    "# Convert 'work_value' to float for calculating 'Project_Cost_Total'\n",
    "LA_city_data_sub['work_value'] = LA_city_data_sub['work_value'].apply(parse_money)\n",
    "# Calculate 'Project_Cost_Total' by summing 'work_value'\n",
    "LA_city_data_sub['Project_Cost_Total'] = LA_city_data_sub[LA_city_data_sub['work_value'] >= 0].groupby('work_order')['work_value'].transform('sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1208f62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the associated data after extracting list of contract numbers in the previous step\n",
    "\n",
    "# Automatic driver installer\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# URL of the site\n",
    "url = 'https://bca.lacity.org/Payments/payments_search.php'\n",
    "\n",
    "# Contract number to search for\n",
    "# contract_numbers= ['E1904038','SZC11765','SZC11922','E170128F','L0249155','E1904256',\n",
    "#                     'SZC11321','E1700415','L0549492','EW40011F','E1907554','L0449473',\n",
    "#                     'L1649430', 'E1907775','E1907703']\n",
    "contract_numbers_list_la = LA_city_data_sub[\"work_order\"].unique()\n",
    "print(\"Total Contracts:\",len(contract_numbers_list_la))\n",
    "contract_numbers = contract_numbers_list_la\n",
    "\n",
    "# Start a new browser session\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url)\n",
    "row_data_list_la = []\n",
    "header_data_la = []\n",
    "\n",
    "try:\n",
    "    for i,value in enumerate(contract_numbers):\n",
    "    \n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, \"query\"))\n",
    "        )\n",
    "\n",
    "        # Find the contract number input box and enter the contract number\n",
    "        contract_number_input = driver.find_element(By.NAME, \"query\")\n",
    "        contract_number_input.clear()\n",
    "        contract_number_input.send_keys(value)\n",
    "        contract_number_input.send_keys(Keys.RETURN)\n",
    "        # Submit the search form\n",
    "        try: \n",
    "            \n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'Payments_Search'))\n",
    "            )\n",
    "            \n",
    "\n",
    "            table = driver.find_element(By.ID, 'Payments_Search').find_element(By.TAG_NAME, 'thead').find_element(By.TAG_NAME, 'tr')\n",
    "            # Extract the table headers\n",
    "            if i == 0: \n",
    "                headers = table.find_elements(By.TAG_NAME, 'th')\n",
    "                header_data_la.extend([header.get_attribute(\"textContent\").strip() for header in headers if header.get_attribute(\"textContent\").strip()])\n",
    "                \n",
    "            if not header_data_la:\n",
    "                headers = table.find_elements(By.TAG_NAME, 'th')\n",
    "                header_data_la.extend([header.get_attribute(\"textContent\").strip() for header in headers if header.get_attribute(\"textContent\").strip()])\n",
    "            set_records = driver.find_element(By.XPATH, '//*[@id=\"Payments_Search_length\"]/label/select/option[4]') # Displays maximum number of records in a page for efficiency\n",
    "            set_records.click()   \n",
    "            # Iterate over all pages of transactions\n",
    "            next_page_button = driver.find_element(By.XPATH,'//div[@id=\"Payments_Search_paginate\"]/a[@id=\"Payments_Search_next\"]')        \n",
    "            while \"disabled\" not in next_page_button.get_attribute(\"class\"):\n",
    "                # Extract all records of transactions for a work order\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'Payments_Search'))\n",
    "                )\n",
    "                rows = driver.find_element(By.ID, 'Payments_Search').find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "                for row in rows:\n",
    "                    cols = row.find_elements(By.TAG_NAME, 'td')  # Find data cells\n",
    "                    row_data = [col.get_attribute(\"textContent\")for col in cols]\n",
    "                    if any(row_data):\n",
    "                        row_data_list_la.append(row_data)\n",
    "                next_page_button.click()\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'Payments_Search_paginate'))\n",
    "                )             \n",
    "                next_page_button = driver.find_element(By.XPATH,'//div[@id=\"Payments_Search_paginate\"]/a[@id=\"Payments_Search_next\"]') \n",
    "                \n",
    "            # Last page needs to be extracted as well\n",
    "            if \"disabled\" in next_page_button.get_attribute(\"class\"):\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'Payments_Search'))\n",
    "                )\n",
    "                rows = driver.find_element(By.ID, 'Payments_Search').find_element(By.TAG_NAME, 'tbody').find_elements(By.TAG_NAME, 'tr')\n",
    "                for row in rows:\n",
    "                    cols = row.find_elements(By.TAG_NAME, 'td')  # Find data cells\n",
    "                    row_data = [col.get_attribute(\"textContent\") for col in cols]\n",
    "                    if any(row_data):\n",
    "                        row_data_list_la.append(row_data)\n",
    "        \n",
    "        except TimeoutException:\n",
    "            print(value,\"Data Not Found\")\n",
    "            \n",
    "        driver.get(url) # Go back to search box\n",
    "        \n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "LA_city_data = pd.DataFrame(data=row_data_list_la, columns = header_data_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34575bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the main table using informaton from sub table\n",
    "# Have only one instance of each work_order from the sub table to prepare for joining\n",
    "LA_city_data_sub_unique = LA_city_data_sub.drop_duplicates(subset=LA_city_data_sub.loc[:, LA_city_data_sub.columns.isin(['work_order'])].columns,keep=\"first\")\n",
    "# Join the sub unique table with the main table in order to get 'project_title', 'Project_Cost_Total'\n",
    "LA_city_data = pd.merge(LA_city_data, LA_city_data_sub_unique, left_on='Work Order', right_on='work_order',how='left')\n",
    "# Drop unnecessary columns\n",
    "LA_city_data.drop(columns=['subcontractor_name','work_description','work_value','work_order'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the main table using informaton from sub table\n",
    "def parse_money(value):\n",
    "    if not value:  # Check for None, empty string, or NaN\n",
    "        return None\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Convert values in parentheses to negative numbers\n",
    "    if '(' in value and ')' in value:\n",
    "        value = '-' + value[1:-1]  # Remove the parentheses and add a negative sign\n",
    "    return float(value) if value.strip() else None\n",
    "\n",
    "\n",
    "# Email configuration\n",
    "SMTP_SERVER = \"smtp.office365.com\"  # e.g., smtp.gmail.com\n",
    "SMTP_PORT = 587  # Usually 587 for TLS\n",
    "EMAIL_SENDER = \"tarun.pongulaty@revealgc.com\"\n",
    "EMAIL_PASSWORD = \"\"  # Use app passwords or secure vaults\n",
    "EMAIL_RECIPIENT = \"tarun.pongulaty@revealgc.com\"\n",
    "\n",
    "def send_email(subject, body):\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg['From'] = EMAIL_SENDER\n",
    "        msg['To'] = EMAIL_RECIPIENT\n",
    "        msg['Subject'] = subject\n",
    "        msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "            server.starttls()\n",
    "            server.login(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "            server.send_message(msg)\n",
    "        print(\"Email notification sent successfully.\")\n",
    "        logging.info(\"Email notification sent successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to send email notification: {e}\")\n",
    "        logging.error(f\"Failed to send email notification: {e}\")\n",
    "\n",
    "# Post Processing\n",
    "df = LA_city_data.copy()\n",
    "df_sub = LA_city_data_sub.copy()\n",
    "df.rename(columns = {'Work Order':'Project_Number','project_title':'Project_Description_Full','Job Title':'Project_Description','Approved by Inspector':'Inspector_Approval',\n",
    "                     'Approved by Supervisor':'Supervisor_Approval','Payment Number':'Payment_Number','Amount':'Payment_Amount','Received by Payments':'Payment_Received',\n",
    "                     'Sent to Accounting':'Payment_toAccounting','Escrow Released':'Escrow_Released'}, inplace=True)\n",
    "df_sub.rename(columns = {'work_order':'Project_Number','project_title':'Project_Description_Full','work_description':'Project_Justification_Sub'}, inplace=True)\n",
    "\n",
    "# Convert data types of columns necessary for further calculations\n",
    "df['Payment_Number'] = df['Payment_Number'].astype(int)                                                        \n",
    "df['Payment_Amount'] = df['Payment_Amount'].apply(parse_money)\n",
    "df['Escrow_Released'] = df['Escrow_Released'].apply(parse_money)\n",
    "\n",
    "EST = pytz.timezone('US/Eastern')\n",
    "now = datetime.now(EST)\n",
    "current_date = now.strftime(\"%m/%d/%Y\")\n",
    "df[\"Pull_Date_Initial\"] = current_date\n",
    "\n",
    "# DUCKDB INTEGRATION\n",
    "# File to store DuckDB data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\LA_City\\data_store_LA.duckdb\"\n",
    "table_name = \"LA_DOT\"\n",
    "table_name_sub = \"LA_DOT_sub\" \n",
    "\n",
    "# Current scraped data\n",
    "scraped_data = df\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(db_file)\n",
    "\n",
    "# Create table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "Project_Number  TEXT,\n",
    "Payment_Number  INTEGER,\n",
    "Project_Description_Full  TEXT,\n",
    "Project_Description\tTEXT,\n",
    "Comments  TEXT,\n",
    "Inspector_Approval  TEXT,\n",
    "Supervisor_Approval  TEXT,\n",
    "\n",
    "Payment_Received  TEXT,\n",
    "Payment_toAccounting  TEXT,\n",
    "\n",
    "Payment_Amount DOUBLE,\t\n",
    "Escrow_Released  DOUBLE,\n",
    "Project_Cost_Total  DOUBLE,\t\n",
    "Payment_Amount_Total  DOUBLE,\n",
    "Payment_Balance  DOUBLE,\n",
    "Payment_Total_Percent  FLOAT,\n",
    "Payment_Amount_Percent FLOAT,\n",
    "Pull_Date_Initial TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data = con.execute(f\"SELECT * FROM {table_name}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data.empty:\n",
    "    combined_data = pd.concat([existing_data, scraped_data], ignore_index=True)\n",
    "    # find duplicates by all columns except the columns below since they are calculated later. Later, a logic can be developed to find revised payments if any\n",
    "    combined_data = combined_data.drop_duplicates(subset=df.loc[:, ~df.columns.isin(['Pull_Date_Initial', 'Payment_Amount_Percent','Payment_Amount_Total','Payment_Total_Percent',\n",
    "                                                                                     'Comments','Inspector_Approval','Supervisor_Approval','Payment_Balance'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    # Sort the DataFrame. Note: Sorting \"Pull_Date_Initial\" isn't required. Only if a payment number is duplicated for some reason.\n",
    "    combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Payment_Number\",\"Pull_Date_Initial\"], ascending=[True,True,False],na_position='first')\n",
    "  \n",
    "    # Calculate the Current Payment Amount Total\n",
    "    combined_data['Payment_Amount_Total'] = combined_data[combined_data['Payment_Amount'] >= 0].groupby(\"Project_Number\")['Payment_Amount'].cumsum()\n",
    "    # Revert the formatting of the date column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment_Balance\n",
    "    combined_data['Payment_Balance'] = combined_data['Project_Cost_Total'] - combined_data['Payment_Amount_Total']\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Calculate Payment Total Percent\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]   \n",
    "\n",
    "# If loading data for the first time\n",
    "else:\n",
    "    combined_data = scraped_data\n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data['Pull_Date_Initial'] = pd.to_datetime(combined_data['Pull_Date_Initial'])\n",
    "    # Sort the DataFrame. Note: Sorting \"Pull_Date_Initial\" isn't required. Only if a payment number is duplicated for some reason.\n",
    "    combined_data = combined_data.sort_values(by=[\"Project_Number\", \"Payment_Number\",\"Pull_Date_Initial\"], ascending=[True,True,False],na_position='first')\n",
    "  \n",
    "    # Calculate the Current Payment Amount Total\n",
    "    combined_data['Payment_Amount_Total'] = combined_data[combined_data['Payment_Amount'] >= 0].groupby(\"Project_Number\")['Payment_Amount'].cumsum()\n",
    "    # Revert the formatting of the date column\n",
    "    combined_data[\"Pull_Date_Initial\"] = combined_data[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    # Calculate Payment_Balance\n",
    "    combined_data['Payment_Balance'] = combined_data['Project_Cost_Total'] - combined_data['Payment_Amount_Total']\n",
    "    # Calculate Payment Amount Percent\n",
    "    combined_data['Payment_Amount_Percent'] = (combined_data['Payment_Amount']/combined_data['Payment_Amount_Total'] * 100).round(2)\n",
    "    # Calculate Payment Total Percent\n",
    "    combined_data['Payment_Total_Percent'] = (combined_data['Payment_Amount_Total']/combined_data['Project_Cost_Total'] * 100).round(2)\n",
    "    table_info = con.execute(f\"DESCRIBE {table_name}\").fetchdf()\n",
    "    correct_order = table_info['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data = combined_data[correct_order]  \n",
    "\n",
    "# Current scraped_sub data\n",
    "df_sub[\"Pull_Date_Initial\"] = current_date\n",
    "scraped_data_sub = df_sub\n",
    "\n",
    "\n",
    "# Create sub_table if not exists\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table_name_sub} (\n",
    "Project_Number  TEXT,\n",
    "Project_Description_Full  TEXT,\n",
    "Project_Justification_Sub  TEXT,\n",
    "subcontractor_name  TEXT,\n",
    "\n",
    "work_value  DOUBLE,\n",
    "Project_Cost_Total  DOUBLE,\t\n",
    "Pull_Date_Initial TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert or Update Logic\n",
    "# Load existing data from DuckDB\n",
    "existing_data_sub = con.execute(f\"SELECT * FROM {table_name_sub}\").df()\n",
    "\n",
    "# Deduplicate and merge\n",
    "if not existing_data_sub.empty:\n",
    "    combined_data_sub = pd.concat([existing_data_sub, scraped_data_sub], ignore_index=True)\n",
    "    # find duplicates by all columns except Pull Date Initial\n",
    "    combined_data_sub = combined_data_sub.drop_duplicates(subset=df_sub.loc[:, ~df_sub.columns.isin(['Pull_Date_Initial'])].columns,keep=\"first\") \n",
    "    # Post processing before loading into duckdb\n",
    "    combined_data_sub['Pull_Date_Initial'] = pd.to_datetime(combined_data_sub['Pull_Date_Initial'])\n",
    "    # Sort the DataFrame. Note: Sorting \"Pull_Date_Initial\" isn't required. Only if a payment number is duplicated for some reason.\n",
    "    combined_data_sub = combined_data_sub.sort_values(by=[\"Project_Number\",\"Pull_Date_Initial\"], ascending=[True,False])\n",
    "    # Revert the formatting of the date column\n",
    "    combined_data_sub[\"Pull_Date_Initial\"] = combined_data_sub[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    table_info_sub = con.execute(f\"DESCRIBE {table_name_sub}\").fetchdf()\n",
    "    correct_order_sub = table_info_sub['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data_sub = combined_data_sub[correct_order_sub]   \n",
    "\n",
    "# If loading data for the first time\n",
    "else:\n",
    "    combined_data_sub = scraped_data_sub\n",
    "   # Post processing before loading into duckdb\n",
    "    combined_data_sub['Pull_Date_Initial'] = pd.to_datetime(combined_data_sub['Pull_Date_Initial'])\n",
    "    # Sort the DataFrame. Note: Sorting \"Pull_Date_Initial\" isn't required. Only if a payment number is duplicated for some reason.\n",
    "    combined_data_sub = combined_data_sub.sort_values(by=[\"Project_Number\",\"Pull_Date_Initial\"], ascending=[True,False])\n",
    "    # Revert the formatting of the date column\n",
    "    combined_data_sub[\"Pull_Date_Initial\"] = combined_data_sub[\"Pull_Date_Initial\"].dt.strftime('%m/%d/%Y')\n",
    "    table_info_sub = con.execute(f\"DESCRIBE {table_name_sub}\").fetchdf()\n",
    "    correct_order_sub = table_info_sub['column_name'].tolist()\n",
    "    # Reorder the DataFrame to avoid conversion errors\n",
    "    combined_data_sub = combined_data_sub[correct_order_sub] \n",
    "\n",
    "# Replace the table with the updated data\n",
    "print(combined_data)\n",
    "con.execute(f\"DELETE FROM {table_name}\")\n",
    "con.execute(f\"INSERT INTO {table_name} SELECT * FROM combined_data\")\n",
    "\n",
    "# Replace the sub_table with the updated data\n",
    "con.execute(f\"DELETE FROM {table_name_sub}\")\n",
    "con.execute(f\"INSERT INTO {table_name_sub} SELECT * FROM combined_data_sub\")\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "send_email(\"Python Script Execution Successful\", \"The job was executed successfully.\")\n",
    "print(\"LA_City scraping completed and DUCKDB file updated Successfully.\")\n",
    "logging.info(\n",
    "    'LA_City scraping completed and DUCKDB file updated Successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46129343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from corresponding duckdb table. Specify State, file and table name respectively\n",
    "data = get_data(\"LA_City\", \"data_store_LA\", \"LA_DOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6517e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\LA_City\\Monthly\\la_city_pipeline_april.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a56d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = get_data(\"LA_City\", \"data_store_LA\", \"LA_DOT_sub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.to_excel(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\LA_City\\Monthly\\la_city_pipeline_sub_april.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19be67",
   "metadata": {},
   "source": [
    "Examples on Alterations to existing DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For renaming columns\n",
    "import duckdb\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Oklahoma\\data_store_ok.duckdb\"\n",
    "# Create an in-memory DuckDB database and connection\n",
    "con = duckdb.connect(db_file)\n",
    "# Rename columns\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN bid_amount TO Project_Cost_Total;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN funds_available_bid_co TO Project_Cost_Adjusted;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN completion_date TO Project_Comp_Final;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN date_time_stopped TO Project_Charges_Stopped;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN surety_company TO Bond_Company_Name;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN prime_cont TO Contractor_Name;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN this_estimate TO Payment_Amount;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN date_approved TO Payment_Approved;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN total_to_date TO Payment_Amount_Total;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN unearned_balance TO Payment_Balance;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN percent_complete TO Payment_Total_Percent;\")\n",
    "\n",
    "# Verify the changes\n",
    "print(con.execute(\"DESCRIBE OK_DOT\").fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc466b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For dropping columns\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Washington\\data_store_wa.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "# # Delete a column\n",
    "con.execute(\"ALTER TABLE WA_DOT DROP COLUMN Project_Total_Percent;\")\n",
    "\n",
    "# Verify the changes\n",
    "print(con.execute(\"DESCRIBE WA_DOT\").fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating existing columns in case of any changes in transformation methods\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Illinois\\data_store_IL.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "# Update the column with the formula\n",
    "con.execute(\"\"\"\n",
    "    UPDATE IL_DOT \n",
    "    SET Payment_Balance = ROUND(Project_Cost_Total_Adjusted - Payment_Amount_Total, 2);\n",
    "\"\"\")\n",
    "# Verify the results\n",
    "print(con.execute(\"SELECT * FROM IL_DOT\").fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56db9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the data types of existing columns in duckdb\n",
    "# Step 3: Add new column with proper type and data\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Oklahoma\\data_store_ok.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN org_cont_time_double DOUBLE;\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN current_time_allowed_double DOUBLE;     \n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    UPDATE OK_DOT\n",
    "    SET\n",
    "        org_cont_time_double = TRY_CAST(org_cont_time AS DOUBLE),\n",
    "        current_time_allowed_double = TRY_CAST(current_time_allowed AS DOUBLE);\n",
    "\"\"\")\n",
    "\n",
    "# Step 4: Drop the original column\n",
    "con.execute(\"ALTER TABLE OK_DOT DROP COLUMN org_cont_time;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT DROP COLUMN current_time_allowed;\")\n",
    "\n",
    "# Step 5: Rename the new column to the original name\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN org_cont_time_double TO org_cont_time;\")\n",
    "con.execute(\"ALTER TABLE OK_DOT RENAME COLUMN current_time_allowed_double TO current_time_allowed;\")\n",
    "\n",
    "# Step 6: Check the result\n",
    "df = con.execute(\"SELECT * FROM OK_DOT;\").fetchdf()\n",
    "print(df.head())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For creating new derived columns\n",
    "db_file = r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Oklahoma\\data_store_ok.duckdb\"\n",
    "con = duckdb.connect(db_file)\n",
    "# 1. Add the new columns\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN Project_Cost_Additions DOUBLE;\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN Payment_Amount_Percent DOUBLE;\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN Payment_Date TEXT;\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN Project_Comp_Est TEXT;\n",
    "\"\"\")\n",
    "con.execute(\"\"\"\n",
    "    ALTER TABLE OK_DOT ADD COLUMN Project_Comp_Substantial TEXT;\n",
    "\"\"\")\n",
    "\n",
    "# 2. Update values using formulas\n",
    "con.execute(\"\"\"\n",
    "    UPDATE OK_DOT\n",
    "    SET \n",
    "        Project_Cost_Additions = TRY_CAST((Project_Cost_Adjusted - Project_Cost_Total) AS DOUBLE),\n",
    "        Payment_Amount_Percent = TRY_CAST(ROUND((Payment_Amount / Payment_Amount_Total) * 100, 2) AS DOUBLE),\n",
    "        Payment_Date = SUBSTR(Payment_Work_Period, LENGTH(Payment_Work_Period) - 9, 10),\n",
    "        Project_Comp_Est = STRFTIME(\n",
    "            STRPTIME(date_ntp_issued, '%m/%d/%Y') + (CAST(org_cont_time AS INTEGER) || ' days')::INTERVAL,\n",
    "            '%m/%d/%Y'\n",
    "        ),\n",
    "        Project_Comp_Substantial = STRFTIME(\n",
    "            STRPTIME(date_ntp_issued, '%m/%d/%Y') + (CAST(current_time_allowed AS INTEGER) || ' days')::INTERVAL,\n",
    "            '%m/%d/%Y'\n",
    "        );\n",
    "\"\"\")\n",
    "\n",
    "# 3. View result\n",
    "df = con.execute(\"SELECT * FROM OK_DOT;\").fetchdf()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5756ef4e",
   "metadata": {},
   "source": [
    "MFT Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3c0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "# census_mft_resource.py\n",
    "This script transfers files to census via MFT\n",
    "\"\"\"\n",
    "\n",
    "import base64\n",
    "import re\n",
    "import subprocess\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class MFTClient:\n",
    "    \"\"\"\n",
    "    A client for sending files to a Managed File Transfer (MFT) server using cURL.\n",
    "\n",
    "    Attributes:\n",
    "        url (str): The base URL of the MFT server.\n",
    "        username (str): The username for authentication.\n",
    "        password (str): The password for authentication.\n",
    "\n",
    "    Methods:\n",
    "        send_file(target_file: Path, dest_name: str, dest_folder=\"\") -> subprocess.CompletedProcess:\n",
    "            Uploads a file to the specified MFT destination folder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str,\n",
    "        token: Optional[str] = None,\n",
    "        username: Optional[str] = None,\n",
    "        password: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the MFTClient with server credentials.\n",
    "\n",
    "        Args:\n",
    "            url (str): The base URL of the MFT server.\n",
    "            username (str): The username for authentication.\n",
    "            password (str): The password for authentication.\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        if token is None and (username is None or password is None):\n",
    "            raise ValueError(\"Either token or username and password must be provided.\")\n",
    "        if token is not None and (username is not None or password is not None):\n",
    "            raise ValueError(\n",
    "                \"Only one of token or username and password can be provided.\"\n",
    "            )\n",
    "\n",
    "        self.token = (\n",
    "            token if token is not None else self._encode_credentials(username, password)\n",
    "        )\n",
    "\n",
    "    def _encode_credentials(self, username, password) -> str:\n",
    "        \"\"\"\n",
    "        Encodes the username and password for use in HTTP Basic Authentication.\n",
    "\n",
    "        Returns:\n",
    "            str: The encoded credentials.\n",
    "        \"\"\"\n",
    "        return base64.b64encode(f\"{username}:{password}\".encode()).decode()\n",
    "\n",
    "    def send_file(self, target_file: Path, dest_name: str, dest_folder=\"\"):\n",
    "        \"\"\"\n",
    "        Uploads a file to the MFT server.\n",
    "\n",
    "        Args:\n",
    "            target_file (Path): The local file path to upload.\n",
    "            dest_name (str): The name the file should have on the MFT server.\n",
    "            dest_folder (str, optional): The destination folder on the MFT server. Defaults to \"\".\n",
    "\n",
    "        Returns:\n",
    "            subprocess.CompletedProcess: The result of the cURL command execution.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the specified file does not exist.\n",
    "            ValueError: If authentication credentials are missing.\n",
    "        \"\"\"\n",
    "\n",
    "        # create user token\n",
    "        file_path = Path(target_file).resolve()\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        folder_url = self.url + dest_folder\n",
    "\n",
    "        # verify file exists\n",
    "\n",
    "        # create curl command\n",
    "        mft_str = (\n",
    "            f'curl -X POST \"{folder_url}/{dest_name}?packet=1&position=0&final=true\" '\n",
    "            f'-H \"Authorization: Basic {self.token}\" '\n",
    "            f'-T \"{target_file}\" -v'\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # send file wit subprocess logging\n",
    "            result = subprocess.run(\n",
    "                mft_str, shell=True, capture_output=True, text=True, check=True\n",
    "            )\n",
    "            success = True\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            result = e\n",
    "            success = False\n",
    "\n",
    "        redacted_stdout = self._scrub_sensitive_data(result.stdout)\n",
    "        redacted_stderr = self._scrub_sensitive_data(result.stderr)\n",
    "\n",
    "        return {\n",
    "            \"stdout\": redacted_stdout,\n",
    "            \"stderr\": redacted_stderr,\n",
    "            \"returncode\": result.returncode,\n",
    "            \"error\": None if success else \"File transfer failed.\",\n",
    "        }\n",
    "\n",
    "    def _scrub_sensitive_data(self, text):\n",
    "        \"\"\"\n",
    "        Removes sensitive information from subprocess output before logging.\n",
    "\n",
    "        Args:\n",
    "            text (str): The original stdout or stderr text.\n",
    "\n",
    "        Returns:\n",
    "            str: The redacted text with authentication details removed.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"No output\"\n",
    "\n",
    "        # Use regex to remove the Authorization header details\n",
    "        redacted_text = re.sub(\n",
    "            r\"Authorization: Basic [A-Za-z0-9+/=]+\",\n",
    "            \"Authorization: Basic ***REDACTED***\",\n",
    "            text,\n",
    "        )\n",
    "\n",
    "        return redacted_text\n",
    "\n",
    "\n",
    "# Example usage of MFTClient\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "\n",
    "    # Load environment variables for credentials (or set manually)\n",
    "    MFT_URL = os.getenv(\n",
    "        \"MFT_BASE_URL\", \"https://mft.econ.census.gov/cfcc/rest/ft/v3/transfer/\"\n",
    "    )\n",
    "    # MFT_USERNAME = os.getenv(\"your_username\", default=\"MFT_USERNAME\")\n",
    "    MFT_USERNAME = \"\"\n",
    "    # MFT_PASSWORD = os.getenv(\"your_password\", default=\"MFT_PASSWORD\")\n",
    "    MFT_PASSWORD = \"\"\n",
    "\n",
    "    # Create an MFTClient instance\n",
    "    mft_client = MFTClient(url=MFT_URL, username=MFT_USERNAME, password=MFT_PASSWORD)\n",
    "\n",
    "    # Define file to upload\n",
    "\n",
    "    try:\n",
    "        # Upload file to MFT server\n",
    "        res = mft_client.send_file(\n",
    "            Path(r\"C:\\Users\\TarunPongulaty\\Documents\\Revealgc\\Reveal_Census - databases\\Tarun\\dot_scraping\\Wisconsin\\wis_dot_api_test8.xlsx\"), \"vip_wisconsin_test\", \"rgc_analyzed\"\n",
    "            )\n",
    "\n",
    "        # Print upload response\n",
    "        print(\"STDOUT:\", res[\"stdout\"])\n",
    "        print(\"STDERR:\", res[\"stderr\"])\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"File not found error:\", e)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Subprocess error:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
